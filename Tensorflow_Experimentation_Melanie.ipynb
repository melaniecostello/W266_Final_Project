{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import HTMLParser as htm\n",
    "import string\n",
    "import re\n",
    "import time\n",
    "\n",
    "# SK-learn library for splitting data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Positive</th>\n",
       "      <th>escape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138881940341260288:</td>\n",
       "      <td>I got a surprise for all you bitches...pull th...</td>\n",
       "      <td>:: surprise</td>\n",
       "      <td>0</td>\n",
       "      <td>I got a surprise for all you bitches...pull th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>144479819843911683:</td>\n",
       "      <td>If I was a thief.. The first thing I would ste...</td>\n",
       "      <td>:: joy</td>\n",
       "      <td>1</td>\n",
       "      <td>If I was a thief.. The first thing I would ste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>139110849120972800:</td>\n",
       "      <td>\"&amp;quot;@RevRunWisdom: not afraid of tomorrow, ...</td>\n",
       "      <td>:: fear</td>\n",
       "      <td>0</td>\n",
       "      <td>\"\"@RevRunWisdom: not afraid of tomorrow, for I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>141532076791971840:</td>\n",
       "      <td>\"Extreme can neither fight nor fly.&amp;#xA;-- Wil...</td>\n",
       "      <td>:: fear</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Extreme can neither fight nor fly.\\n-- Willia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>145353048817012736:</td>\n",
       "      <td>Thinks that @melbahughes had a great 50th birt...</td>\n",
       "      <td>:: surprise</td>\n",
       "      <td>0</td>\n",
       "      <td>Thinks that @melbahughes had a great 50th birt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Id                                              Tweet  \\\n",
       "0  138881940341260288:  I got a surprise for all you bitches...pull th...   \n",
       "1  144479819843911683:  If I was a thief.. The first thing I would ste...   \n",
       "2  139110849120972800:  \"&quot;@RevRunWisdom: not afraid of tomorrow, ...   \n",
       "3  141532076791971840:  \"Extreme can neither fight nor fly.&#xA;-- Wil...   \n",
       "4  145353048817012736:  Thinks that @melbahughes had a great 50th birt...   \n",
       "\n",
       "       Emotion  Positive                                             escape  \n",
       "0  :: surprise         0  I got a surprise for all you bitches...pull th...  \n",
       "1       :: joy         1  If I was a thief.. The first thing I would ste...  \n",
       "2      :: fear         0  \"\"@RevRunWisdom: not afraid of tomorrow, for I...  \n",
       "3      :: fear         0  \"Extreme can neither fight nor fly.\\n-- Willia...  \n",
       "4  :: surprise         0  Thinks that @melbahughes had a great 50th birt...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"tweet_data_1.csv\",sep='\\t',quoting=3)\n",
    "data[\"escape\"] = data.apply(lambda row: htm.HTMLParser().unescape(row[1].decode(\"utf-8\")),axis=1)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train & test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train and test data frames\n",
    "train, test = train_test_split(data, test_size = 0.2)\n",
    "\n",
    "# Train and test target labels for polarity\n",
    "train_pol_y = train.ix[:,3].tolist()\n",
    "test_pol_y = test.ix[:,3].tolist()\n",
    "\n",
    "# Binarize labels for sub-emotion classifier\n",
    "train_emo = train.ix[:,2].tolist()\n",
    "test_emo = test.ix[:,2].tolist()\n",
    "emo_bin = preprocessing.LabelBinarizer()\n",
    "\n",
    "# Labels for sub-emotion classifier\n",
    "train_emo_y = emo_bin.fit_transform(train_emo)\n",
    "tests_emo_y = emo_bin.transform(test_emo)\n",
    "\n",
    "# Train and test inputs\n",
    "train_pol_x = train.ix[:, 4].tolist()\n",
    "test_pol_x = test.ix[:, 4].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_emo_y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get matrix ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Matrix ids for each tweet were built using GloVe word embeddings\n",
    "# Because construction of matrix ids is computationally expensive,\n",
    "# matrix ids were saved and will simply be reloaded\n",
    "d = np.load('ids.npz')\n",
    "train_ids = d['train_ids']\n",
    "test_ids = d['test_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull in GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the word list!\n",
      "Loaded the word vectors!\n"
     ]
    }
   ],
   "source": [
    "# Pull in word list & vectors\n",
    "wordsList = np.load('wordsList.npy')\n",
    "print('Loaded the word list!')\n",
    "wordsList = wordsList.tolist() #Originally loaded as numpy array\n",
    "wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "wordVectors = np.load('wordVectors.npy')\n",
    "print ('Loaded the word vectors!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "# For Polarity Classifier\n",
    "def getTrainBatch(train_data, train_labels, train_ids):\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    # iterate through batch size\n",
    "    for i in range(batchSize):\n",
    "        num = randint(1, (len(train_data)-1))\n",
    "        if train_labels[num-1] == 1:\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "            \n",
    "        arr[i] = train_ids[num-1:num]\n",
    "        \n",
    "    return arr.astype(int), labels\n",
    "\n",
    "def getTestBatch(test_data, test_labels, test_ids):\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(1,(len(test_data)-1))\n",
    "        \n",
    "        if test_labels[num-1] == 1:\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "            \n",
    "        arr[i] = test_ids[num-1:num]\n",
    "        \n",
    "    return arr.astype(int), labels\n",
    "\n",
    "# For sub-emotion classifier\n",
    "def getTrainBatch_subEmo(train_data, train_labels, train_ids, batchSize, maxSeqLength):\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    # iterate through batch size\n",
    "    for i in range(batchSize):\n",
    "        num = randint(1, (len(train_data)-1))\n",
    "        labels.append(train_labels[num-1])\n",
    "            \n",
    "        arr[i] = train_ids[num-1:num]\n",
    "        \n",
    "    return arr.astype(int), labels\n",
    "\n",
    "\n",
    "def getTestBatch_subEmo(test_data, test_labels, test_ids, batchSize, maxSeqLength):\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(1,(len(test_data)-1))\n",
    "        labels.append(test_labels[num-1])\n",
    "            \n",
    "        arr[i] = test_ids[num-1:num]\n",
    "        \n",
    "    return arr.astype(int), labels\n",
    "\n",
    "def matmul3d(X, W):\n",
    "    \"\"\"Wrapper for tf.matmul to handle a 3D input tensor X.\n",
    "    Will perform multiplication along the last dimension.\n",
    "    Args:\n",
    "      X: [m,n,k]\n",
    "      W: [k,l]\n",
    "    Returns:\n",
    "      XW: [m,n,l]\n",
    "    \"\"\"\n",
    "    Xr = tf.reshape(X, [-1, tf.shape(X)[2]])\n",
    "    XWr = tf.matmul(Xr, W)\n",
    "    newshape = [tf.shape(X)[0], tf.shape(X)[1], tf.shape(W)[1]]\n",
    "    return tf.reshape(XWr, newshape)\n",
    "\n",
    "\n",
    "def MakeFancyRNNCell(H, keep_prob, num_layers=1):\n",
    "    \"\"\"Make a fancy RNN cell.\n",
    "    Use tf.nn.rnn_cell functions to construct an LSTM cell.\n",
    "    Initialize forget_bias=0.0 for better training.\n",
    "    Args:\n",
    "      H: hidden state size\n",
    "      keep_prob: dropout keep prob (same for input and output)\n",
    "      num_layers: number of cell layers\n",
    "    Returns:\n",
    "      (tf.nn.rnn_cell.RNNCell) multi-layer LSTM cell with dropout\n",
    "    \"\"\"\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(H, forget_bias=0.0)\n",
    "    cell = tf.contrib.rnn.DropoutWrapper(\n",
    "        cell, input_keep_prob=keep_prob, output_keep_prob=keep_prob)\n",
    "#     cell = tf.contrib.rnn.MultiRNNCell([cell for _ in range(num_layers)])\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers)\n",
    "    return cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub-emotion Classifier without polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Model\n",
    "\n",
    "Changes: \n",
    "- adding forget_bias to the LSTM Cell\n",
    "- adding keep_prob\n",
    "\n",
    "Check on:\n",
    "- use of tf.nn.dynamic_rnn cell vs MultiRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Output_Layer/Add:0\", shape=(75, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Specify parameters\n",
    "# maxSeqLength = max([len(elem.split()) for elem in data.ix[:, 4]])\n",
    "maxSeqLength = 31\n",
    "#Maximum number of words in a tweet\n",
    "batchSize = 75\n",
    "hiddenStateSize = 1\n",
    "# lstmUnits = 2\n",
    "numClasses = 6\n",
    "numDimensions = 50\n",
    "keepProb = 0.5\n",
    "learningRate = 0.001\n",
    "\n",
    "iterations = 250\n",
    "\n",
    "# Reset graph & create placeholders\n",
    "tf.reset_default_graph()\n",
    "labels = tf.placeholder(tf.int32, [batchSize, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])\n",
    "\n",
    "##ADD NS on 8/2\n",
    "ns = tf.tile([maxSeqLength], [batchSize, ])\n",
    "\n",
    "# Lookup word vectors\n",
    "with tf.name_scope(\"Embedding_Layer\"):\n",
    "    data_vec = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "    data_vec = tf.nn.embedding_lookup(wordVectors,input_data)\n",
    "#     print \"Embedding Layer shape\", data_vec.shape\n",
    "\n",
    "# Construct RNN/LSTM cell and recurrent layer.\n",
    "with tf.name_scope(\"Cell_RNN_Layer\"):\n",
    "    cells=[]\n",
    "    for _ in range(hiddenStateSize):\n",
    "        lstmCell = tf.contrib.rnn.BasicLSTMCell(numDimensions, forget_bias=0.0)\n",
    "        lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, input_keep_prob=keepProb, output_keep_prob=keepProb)        \n",
    "        cells.append(lstmCell)\n",
    "        multicell = tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
    "    value, _ = tf.nn.dynamic_rnn(multicell, data_vec, sequence_length=ns, dtype=tf.float32)\n",
    "# with tf.name_scope(\"Cell_RNN_Layer\"):\n",
    "#     lstmCell = tf.contrib.rnn.BasicLSTMCell(numDimensions, forget_bias=0.0)\n",
    "#     lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, input_keep_prob=keepProb, output_keep_prob=keepProb)            \n",
    "#     lstmCell = tf.contrib.rnn.MultiRNNCell([lstmCell] * hiddenStateSize)\n",
    "#     value, _ = tf.nn.dynamic_rnn(lstmCell, data_vec, dtype=tf.float32)\n",
    "#     print \"Output of RNN shape\", value.shape\n",
    "    \n",
    "with tf.name_scope(\"Output_Layer\"):\n",
    "    weight = tf.Variable(tf.random_uniform([numDimensions, numClasses], -1.0, 1.0))\n",
    "    bias = tf.Variable(tf.zeros(numClasses, tf.float32))\n",
    "    value = tf.transpose(value, [1, 0, 2])\n",
    "    last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "    multiplier = tf.matmul(last, weight)\n",
    "    prediction = tf.add(multiplier, bias)\n",
    "    print prediction\n",
    " \n",
    "#     print \"Weights shape\", weight.shape\n",
    "#     print \"Bias shape\", bias.shape\n",
    "#     print \"New shape for value\", value.shape\n",
    "#     print \"last shape\", last.shape\n",
    "#     print \"multiplier shape\", multiplier.shape\n",
    "#     print \"Output shape\", prediction.shape\n",
    "\n",
    "    \n",
    "# From A3\n",
    "#     multiplier = matmul3d(value, weight)\n",
    "#     print \"Multiplier shape\", multiplier.shape\n",
    "#     prediction = tf.add(multiplier, bias)\n",
    "#     print \"Logits shape\", prediction.shape\n",
    "    \n",
    "with tf.name_scope(\"Prediction_Layer\"):\n",
    "    # Define correct predictions and accuracy\n",
    "    comparison = tf.argmax(prediction,1)\n",
    "    correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "\n",
    "    # Define loss & optimizer\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learningRate).minimize(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ignore - Scratch paper notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with tf.name_scope(\"Output_Layer\"):\n",
    "            \n",
    "#     # W_out_ is the transpose of W_in_\n",
    "#     self.W_out_ = tf.transpose(self.W_in_)\n",
    "            \n",
    "#     # Initialize b_out_\n",
    "#     self.b_out_ = tf.zeros(self.V, tf.float32, name=\"b_out_\")\n",
    "\n",
    "#     # Logits will be of (batch size, max time, V)\n",
    "#     self.logits_ = tf.add(matmul3d(self.output_, self.W_out_), self.b_out_)\n",
    "            \n",
    "            \n",
    "#     # Loss computation (true loss, for prediction)\n",
    "#     self.loss_one_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.target_y_, logits=self.logits_)\n",
    "#     self.loss_ = tf.reduce_mean(self.loss_one_)\n",
    "    \n",
    "# with tf.name_scope(\"Training_Layer\"):\n",
    "#     # Loss computation (sampled, for training)\n",
    "#     self.loss_step_one_ = tf.nn.sampled_softmax_loss(weights=tf.transpose(self.W_out_), biases=self.b_out_, \n",
    "#                                                      labels=tf.reshape(self.target_y_, [-1, 1]), \n",
    "#                                                      inputs=tf.reshape(self.output_, [-1, self.H]), \n",
    "#                                                      num_sampled=self.softmax_ns, num_classes=self.V)\n",
    "#     self.train_loss_ = tf.reduce_mean(self.loss_step_one_)\n",
    "            \n",
    "#     # Define optimizer and training op\n",
    "#     self.train_step_ = tf.train.AdagradOptimizer(learning_rate=self.learning_rate_).minimize(self.train_loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Specify parameters\n",
    "maxSeqLength = max([len(elem.split()) for elem in data.ix[:, 4]]) #Maximum number of words in a tweet\n",
    "batchSize = 150\n",
    "hiddenStateSize = 1\n",
    "# lstmUnits = 2\n",
    "numClasses = 6\n",
    "numDimensions = 50\n",
    "keepProb = 0.5\n",
    "learningRate = 0.001\n",
    "\n",
    "iterations = 1500\n",
    "\n",
    "# Reset graph & create placeholders\n",
    "tf.reset_default_graph()\n",
    "labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])\n",
    "\n",
    "# Lookup word vectors\n",
    "data_vec = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "data_vec = tf.nn.embedding_lookup(wordVectors,input_data)\n",
    "\n",
    "# Feed RNN cell\n",
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(hiddenStateSize, forget_bias=0.0)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, input_keep_prob=keepProb, output_keep_prob=keepProb)\n",
    "value, _ = tf.nn.dynamic_rnn(lstmCell, data_vec, dtype=tf.float32)\n",
    "\n",
    "# Get final output\n",
    "weight = tf.Variable(tf.truncated_normal([hiddenStateSize, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "value = tf.transpose(value, [1, 0, 2])\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "prediction = (tf.matmul(last, weight) + bias)\n",
    "\n",
    "# Define correct predictions and accuracy\n",
    "comparison = tf.argmax(prediction,1)\n",
    "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "\n",
    "# Define loss & optimizer\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learningRate).minimize(loss)\n",
    "# optimizer = tf.train.AdagradOptimizer(learning_rate=learningRate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (31) into shape (34)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-43c82c913c70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Next Batch of reviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mnextBatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnextBatchLabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetTrainBatch_subEmo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pol_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_emo_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxSeqLength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtrain_inds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtrain_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnextBatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnextBatchLabels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-c6694d54aae0>\u001b[0m in \u001b[0;36mgetTrainBatch_subEmo\u001b[0;34m(train_data, train_labels, train_ids, batchSize, maxSeqLength)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (31) into shape (34)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_inds = []\n",
    "train_logits = []\n",
    "train_labels = []\n",
    "for i in range(iterations):\n",
    "    # Next Batch of reviews\n",
    "    nextBatch, nextBatchLabels,train_i = getTrainBatch_subEmo(train_pol_x, train_emo_y, train_ids, batchSize, maxSeqLength);\n",
    "    train_inds.append(train_i)\n",
    "    train_logs = sess.run([prediction,optimizer], {input_data: nextBatch, labels: nextBatchLabels})\n",
    "    train_logits.append(train_logs[0])\n",
    "    train_labels.append(nextBatchLabels)\n",
    "    # Write summary to Tensorboard\n",
    "    summary = sess.run(merged, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "    writer.add_summary(summary, i)\n",
    "\n",
    "\n",
    "# for i in range(iterations):\n",
    "#     # Next Batch of reviews\n",
    "#     nextBatch, nextBatchLabels = getTrainBatch_subEmo(train_pol_x, train_emo_y, train_ids, batchSize, maxSeqLength);\n",
    "#     sess.run(optimizer, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "   \n",
    "#     # Write summary to Tensorboard\n",
    "#     summary = sess.run(merged, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "#     writer.add_summary(summary, i)\n",
    "\n",
    "#     # Save the network every 10,000 training iterations\n",
    "#     if (i % 10000 == 0 and i != 0):\n",
    "#         save_path = saver.save(sess, \"models/pretrained_lstm.ckpt\", global_step=i)\n",
    "#         print(\"saved to %s\" % save_path)\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iterations = 500\n",
    "l_predictions = []\n",
    "l_labels = []\n",
    "l_logits = []\n",
    "l_inds = []\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels,test_i = getTestBatch_subEmo(test_pol_x, tests_emo_y, test_ids, batchSize, maxSeqLength)\n",
    "\n",
    "    test_log,p,q= (sess.run([prediction,comparison,accuracy], {input_data: nextBatch, labels: nextBatchLabels}))\n",
    "    l_predictions.append(p)\n",
    "    l_labels.append(nextBatchLabels)\n",
    "    l_logits.append(test_log)\n",
    "    l_inds.append(test_i)\n",
    "    #print(\"Accuracy for this batch:\",q)\n",
    "\n",
    "#     print(\"Accuracy for this batch:\", (sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "\n",
    "target_names = emo_bin.classes_.tolist()\n",
    "def score(preds,labels,target_names, indexes):\n",
    "    predictions = np.asarray(preds).ravel()\n",
    "    labels = np.argmax(np.asarray(labels),2).ravel()\n",
    "    indexes = np.asarray(indexes).ravel()\n",
    "    \n",
    "    print classification_report(labels,predictions,target_names=target_names)\n",
    "    \n",
    "    errors = dict()\n",
    "    examples = dict()\n",
    "    for i, p in enumerate(predictions):\n",
    "        if p != labels[i]:\n",
    "            if (p, labels[i]) not in errors:\n",
    "                errors[(p, labels[i])] = 1\n",
    "                examples[(p, labels[i])] = [indexes[i]]\n",
    "            else:\n",
    "                errors[(p, labels[i])] += 1 \n",
    "                examples[(p, labels[i])].append(indexes[i])\n",
    "                \n",
    "    return OrderedDict(sorted(errors.items(), key=itemgetter(1))), examples\n",
    "err, ex = score(l_predictions,l_labels,target_names,l_inds)\n",
    "\n",
    "# See which pairs are getting confused most often\n",
    "for key, val in err.iteritems():\n",
    "    print key, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Compare:', array([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3]))\n"
     ]
    }
   ],
   "source": [
    "# Show index of predicted class\n",
    "print(\"Compare:\", (sess.run(comparison, {input_data: nextBatch, labels: nextBatchLabels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Preds:', array([[-0.014025  ,  0.06395618,  0.12048888,  0.38174605,  0.00953795,\n",
      "         0.07971515],\n",
      "       [-0.00301381,  0.08063177,  0.21235675,  0.44487906, -0.03570636,\n",
      "         0.11071764],\n",
      "       [ 0.02004176,  0.02027215,  0.12705526,  0.25619584,  0.09074374,\n",
      "         0.15652873],\n",
      "       [ 0.01999545,  0.02020205,  0.12666899,  0.25593045,  0.09093395,\n",
      "         0.15639834],\n",
      "       [ 0.02003685,  0.02026472,  0.12701431,  0.25616771,  0.0907639 ,\n",
      "         0.1565149 ],\n",
      "       [-0.01359886,  0.06284045,  0.11891198,  0.37825832,  0.01187535,\n",
      "         0.08056186],\n",
      "       [-0.00301309,  0.08062685,  0.21234521,  0.4448629 , -0.03569534,\n",
      "         0.11071844],\n",
      "       [ 0.00902759,  0.0035997 ,  0.03518479,  0.19307154,  0.13598254,\n",
      "         0.12551938],\n",
      "       [ 0.00902759,  0.0035997 ,  0.03518479,  0.19307154,  0.13598254,\n",
      "         0.12551938],\n",
      "       [-0.00301183,  0.08061852,  0.2123259 ,  0.44483566, -0.03567678,\n",
      "         0.11071994],\n",
      "       [-0.01395453,  0.06377167,  0.1202281 ,  0.38116929,  0.0099245 ,\n",
      "         0.07985517],\n",
      "       [-0.00284813,  0.07965933,  0.2101739 ,  0.44170892, -0.03354707,\n",
      "         0.11093884],\n",
      "       [ 0.00902759,  0.0035997 ,  0.03518479,  0.19307154,  0.13598254,\n",
      "         0.12551938],\n",
      "       [ 0.00902759,  0.0035997 ,  0.03518479,  0.19307154,  0.13598254,\n",
      "         0.12551938],\n",
      "       [-0.00301404,  0.08063333,  0.21236041,  0.44488412, -0.03570983,\n",
      "         0.11071738],\n",
      "       [ 0.02004093,  0.02027089,  0.12704828,  0.25619107,  0.09074716,\n",
      "         0.15652637],\n",
      "       [ 0.02004079,  0.02027069,  0.12704717,  0.2561903 ,  0.09074771,\n",
      "         0.156526  ],\n",
      "       [-0.00301354,  0.08063003,  0.21235272,  0.44487333, -0.03570247,\n",
      "         0.11071796],\n",
      "       [ 0.01998109,  0.02018031,  0.12654918,  0.25584814,  0.09099293,\n",
      "         0.15635791],\n",
      "       [-0.01407713,  0.06409266,  0.12068176,  0.3821727 ,  0.00925204,\n",
      "         0.07961158],\n",
      "       [ 0.00902759,  0.0035997 ,  0.03518479,  0.19307154,  0.13598254,\n",
      "         0.12551938],\n",
      "       [ 0.00902759,  0.0035997 ,  0.03518479,  0.19307154,  0.13598254,\n",
      "         0.12551938],\n",
      "       [ 0.02003549,  0.02026266,  0.12700295,  0.2561599 ,  0.09076949,\n",
      "         0.15651107],\n",
      "       [-0.00300229,  0.08055129,  0.2121675 ,  0.44461527, -0.03552634,\n",
      "         0.11073045],\n",
      "       [-0.01402753,  0.0639628 ,  0.12049824,  0.3817668 ,  0.00952408,\n",
      "         0.07971013],\n",
      "       [-0.00301382,  0.08063187,  0.21235698,  0.44487935, -0.03570655,\n",
      "         0.11071762],\n",
      "       [-0.0030107 ,  0.08061092,  0.2123082 ,  0.44481078, -0.03565978,\n",
      "         0.11072127],\n",
      "       [-0.01401049,  0.06391818,  0.12043517,  0.38162726,  0.00961757,\n",
      "         0.07974399],\n",
      "       [ 0.02002085,  0.02024049,  0.12688078,  0.25607598,  0.09082966,\n",
      "         0.15646984],\n",
      "       [-0.00315416,  0.08157699,  0.21455985,  0.44797352, -0.03781743,\n",
      "         0.11055462],\n",
      "       [ 0.00902759,  0.0035997 ,  0.03518479,  0.19307154,  0.13598254,\n",
      "         0.12551938],\n",
      "       [ 0.00902759,  0.0035997 ,  0.03518479,  0.19307154,  0.13598254,\n",
      "         0.12551938],\n",
      "       [ 0.01964094,  0.01966542,  0.12371197,  0.25389868,  0.09239003,\n",
      "         0.15540026],\n",
      "       [-0.01398474,  0.06385078,  0.1203399 ,  0.38141656,  0.00975879,\n",
      "         0.07979514],\n",
      "       [ 0.00902759,  0.0035997 ,  0.03518479,  0.19307154,  0.13598254,\n",
      "         0.12551938],\n",
      "       [-0.01402543,  0.0639573 ,  0.12049045,  0.38174957,  0.00953561,\n",
      "         0.0797143 ],\n",
      "       [ 0.00902759,  0.0035997 ,  0.03518479,  0.19307154,  0.13598254,\n",
      "         0.12551938],\n",
      "       [-0.00301465,  0.08063775,  0.2123709 ,  0.44489866, -0.03571974,\n",
      "         0.11071673],\n",
      "       [ 0.00902759,  0.0035997 ,  0.03518479,  0.19307154,  0.13598254,\n",
      "         0.12551938],\n",
      "       [ 0.02004062,  0.02027042,  0.12704572,  0.25618932,  0.09074843,\n",
      "         0.15652551],\n",
      "       [ 0.00902759,  0.0035997 ,  0.03518479,  0.19307154,  0.13598254,\n",
      "         0.12551938],\n",
      "       [-0.01401897,  0.06394039,  0.12046657,  0.3816967 ,  0.00957103,\n",
      "         0.07972713],\n",
      "       [ 0.00902759,  0.0035997 ,  0.03518479,  0.19307154,  0.13598254,\n",
      "         0.12551938],\n",
      "       [ 0.02004093,  0.02027089,  0.12704831,  0.25619107,  0.09074716,\n",
      "         0.15652639],\n",
      "       [ 0.00902759,  0.0035997 ,  0.03518479,  0.19307154,  0.13598254,\n",
      "         0.12551938],\n",
      "       [ 0.00902759,  0.0035997 ,  0.03518479,  0.19307154,  0.13598254,\n",
      "         0.12551938],\n",
      "       [ 0.02003781,  0.02026617,  0.12702227,  0.25617319,  0.09075998,\n",
      "         0.1565176 ],\n",
      "       [-0.01115449,  0.0564406 ,  0.10986681,  0.35825232,  0.02528284,\n",
      "         0.08541869],\n",
      "       [-0.01386525,  0.06353793,  0.11989775,  0.38043863,  0.01041418,\n",
      "         0.08003256],\n",
      "       [-0.00301411,  0.08063383,  0.2123616 ,  0.44488579, -0.03571096,\n",
      "         0.1107173 ],\n",
      "       [ 0.00902759,  0.0035997 ,  0.03518479,  0.19307154,  0.13598254,\n",
      "         0.12551938],\n",
      "       [-0.00301271,  0.08062425,  0.21233916,  0.44485441, -0.03568953,\n",
      "         0.11071891],\n",
      "       [-0.00301533,  0.08067965,  0.21249029,  0.4450393 , -0.03581659,\n",
      "         0.11072341],\n",
      "       [ 0.01990128,  0.02005949,  0.12588343,  0.2553907 ,  0.09132077,\n",
      "         0.1561332 ],\n",
      "       [ 0.01772778,  0.01676941,  0.10775407,  0.24293399,  0.10024802,\n",
      "         0.15001392],\n",
      "       [-0.00299326,  0.08050834,  0.21207783,  0.44447637, -0.03543201,\n",
      "         0.11074452],\n",
      "       [-0.01402862,  0.06396564,  0.12050225,  0.38177565,  0.00951813,\n",
      "         0.07970797],\n",
      "       [-0.01402528,  0.06395691,  0.12048991,  0.38174835,  0.00953643,\n",
      "         0.0797146 ],\n",
      "       [-0.01387593,  0.06356589,  0.11993726,  0.38052601,  0.01035561,\n",
      "         0.08001134],\n",
      "       [-0.00305547,  0.08092194,  0.21303865,  0.44582993, -0.03635527,\n",
      "         0.11067116],\n",
      "       [ 0.00902759,  0.0035997 ,  0.03518479,  0.19307154,  0.13598254,\n",
      "         0.12551938],\n",
      "       [ 0.0200326 ,  0.02025828,  0.12697878,  0.25614333,  0.09078139,\n",
      "         0.15650292],\n",
      "       [-0.00300581,  0.08057916,  0.21223485,  0.44470692, -0.03558895,\n",
      "         0.11072718],\n",
      "       [-0.00301291,  0.08062549,  0.21234199,  0.44485846, -0.0356923 ,\n",
      "         0.11071864],\n",
      "       [-0.01361799,  0.06289053,  0.11898275,  0.37841484,  0.01177046,\n",
      "         0.08052386],\n",
      "       [ 0.00902759,  0.0035997 ,  0.03518479,  0.19307154,  0.13598254,\n",
      "         0.12551938],\n",
      "       [-0.0030141 ,  0.08063374,  0.21236137,  0.44488549, -0.03571077,\n",
      "         0.11071731],\n",
      "       [ 0.0185551 ,  0.01802174,  0.11465479,  0.24767548,  0.09684997,\n",
      "         0.15234315],\n",
      "       [ 0.02002932,  0.02025332,  0.12695149,  0.25612456,  0.09079483,\n",
      "         0.15649369],\n",
      "       [ 0.00902759,  0.0035997 ,  0.03518479,  0.19307154,  0.13598254,\n",
      "         0.12551938],\n",
      "       [ 0.01999508,  0.02020149,  0.12666589,  0.25592834,  0.09093547,\n",
      "         0.1563973 ],\n",
      "       [-0.01352322,  0.06264242,  0.11863209,  0.37763926,  0.01229023,\n",
      "         0.08071215],\n",
      "       [ 0.02002329,  0.02024419,  0.12690115,  0.25608996,  0.09081963,\n",
      "         0.15647671],\n",
      "       [-0.01353861,  0.0626827 ,  0.11868902,  0.37776515,  0.01220586,\n",
      "         0.08068159],\n",
      "       [-0.00301392,  0.08063238,  0.21235812,  0.44488102, -0.03570771,\n",
      "         0.1107175 ]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "# Show actual predicted values\n",
    "print(\"Preds:\", (sess.run(prediction, {input_data: nextBatch, labels: nextBatchLabels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tests_emo_y[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
