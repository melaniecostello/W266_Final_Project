{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import HTMLParser as htm\n",
    "import string\n",
    "import re\n",
    "import time\n",
    "\n",
    "# SK-learn library for splitting data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Positive</th>\n",
       "      <th>escape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138881940341260288:</td>\n",
       "      <td>I got a surprise for all you bitches...pull th...</td>\n",
       "      <td>:: surprise</td>\n",
       "      <td>0</td>\n",
       "      <td>I got a surprise for all you bitches...pull th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>144479819843911683:</td>\n",
       "      <td>If I was a thief.. The first thing I would ste...</td>\n",
       "      <td>:: joy</td>\n",
       "      <td>1</td>\n",
       "      <td>If I was a thief.. The first thing I would ste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>139110849120972800:</td>\n",
       "      <td>\"&amp;quot;@RevRunWisdom: not afraid of tomorrow, ...</td>\n",
       "      <td>:: fear</td>\n",
       "      <td>0</td>\n",
       "      <td>\"\"@RevRunWisdom: not afraid of tomorrow, for I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>141532076791971840:</td>\n",
       "      <td>\"Extreme can neither fight nor fly.&amp;#xA;-- Wil...</td>\n",
       "      <td>:: fear</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Extreme can neither fight nor fly.\\n-- Willia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>145353048817012736:</td>\n",
       "      <td>Thinks that @melbahughes had a great 50th birt...</td>\n",
       "      <td>:: surprise</td>\n",
       "      <td>0</td>\n",
       "      <td>Thinks that @melbahughes had a great 50th birt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Id                                              Tweet  \\\n",
       "0  138881940341260288:  I got a surprise for all you bitches...pull th...   \n",
       "1  144479819843911683:  If I was a thief.. The first thing I would ste...   \n",
       "2  139110849120972800:  \"&quot;@RevRunWisdom: not afraid of tomorrow, ...   \n",
       "3  141532076791971840:  \"Extreme can neither fight nor fly.&#xA;-- Wil...   \n",
       "4  145353048817012736:  Thinks that @melbahughes had a great 50th birt...   \n",
       "\n",
       "       Emotion  Positive                                             escape  \n",
       "0  :: surprise         0  I got a surprise for all you bitches...pull th...  \n",
       "1       :: joy         1  If I was a thief.. The first thing I would ste...  \n",
       "2      :: fear         0  \"\"@RevRunWisdom: not afraid of tomorrow, for I...  \n",
       "3      :: fear         0  \"Extreme can neither fight nor fly.\\n-- Willia...  \n",
       "4  :: surprise         0  Thinks that @melbahughes had a great 50th birt...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"tweet_data_1.csv\",sep='\\t',quoting=3)\n",
    "data[\"escape\"] = data.apply(lambda row: htm.HTMLParser().unescape(row[1].decode(\"utf-8\")),axis=1)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(326283, 57028)\n"
     ]
    }
   ],
   "source": [
    "def count_words(df, col):\n",
    "    new_col = []\n",
    "    for elem in df[col]:\n",
    "        for e in elem.split():\n",
    "            new_col.append(e)\n",
    "    return len(new_col), len(set(new_col))\n",
    "\n",
    "x = count_words(data, 'escape')\n",
    "print x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    \"\"\"Converts to lowercase, strips out punctuation,\n",
    "    removes excess whitespace within a string & leading & trailing whitespace\"\"\"\n",
    "    new_list = []\n",
    "    table = string.maketrans(\"\",\"\")\n",
    "    for elem in data:\n",
    "        elem = \"\".join(i for i in elem if ord(i)<128)\n",
    "        elem = str(elem)        \n",
    "        elem = elem.lower()\n",
    "        elem = elem.translate(table, string.punctuation)\n",
    "        elem = re.sub(' +',' ', elem)\n",
    "        elem = elem.strip()\n",
    "        \n",
    "        new_list.append(elem)\n",
    "    return new_list\n",
    "\n",
    "#train_pol_x = process_data(train_pol_x)\n",
    "#test_pol_x = process_data(test_pol_x)\n",
    "\n",
    "#Clean entire data set at once\n",
    "data.escape = process_data(data.escape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(316929, 33813)\n"
     ]
    }
   ],
   "source": [
    "x2 = count_words(data, 'escape')\n",
    "print x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train & test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train and test data frames\n",
    "train, test = train_test_split(data, test_size = 0.2)\n",
    "\n",
    "# Train and test target labels for polarity\n",
    "train_pol_y = train.ix[:,3].tolist()\n",
    "test_pol_y = test.ix[:,3].tolist()\n",
    "\n",
    "# Binarize labels for sub-emotion classifier\n",
    "train_emo = train.ix[:,2].tolist()\n",
    "test_emo = test.ix[:,2].tolist()\n",
    "emo_bin = preprocessing.LabelBinarizer()\n",
    "\n",
    "\n",
    "# Labels for sub-emotion classifier\n",
    "train_emo_y = emo_bin.fit_transform(train_emo)\n",
    "tests_emo_y = emo_bin.transform(test_emo)\n",
    "\n",
    "# Train and test inputs\n",
    "train_pol_x = train.ix[:, 4].tolist()\n",
    "test_pol_x = test.ix[:, 4].tolist()\n",
    "\n",
    "\n",
    "#save data to recall later\n",
    "#np.savez('train_test.npz', train_pol_y=train_pol_y, test_pol_y=test_pol_y,train_pol_x=train_pol_x,\\\n",
    "        #test_pol_x=test_pol_x, train_emo=train_emo,test_emo=test_emo,train_emo_y=train_emo_y,\\\n",
    "        #tests_emo_y=tests_emo_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull in GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the word list!\n",
      "Loaded the word vectors!\n"
     ]
    }
   ],
   "source": [
    "# Pull in word list & vectors\n",
    "wordsList = np.load('wordsList.npy')\n",
    "print('Loaded the word list!')\n",
    "wordsList = wordsList.tolist() #Originally loaded as numpy array\n",
    "wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "wordVectors = np.load('wordVectors.npy')\n",
    "print ('Loaded the word vectors!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxSeqLength = max([len(elem.split()) for elem in data.ix[:, 4]]) #Maximum number of words in a tweet\n",
    "\n",
    "\n",
    "def get_matrix_ids(data, maxSeqLength):\n",
    "    numFiles = len(data)\n",
    "    ids = np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
    "\n",
    "    for fileCounter, tweet in enumerate(data):\n",
    "        start = time.time()\n",
    "        split = tweet.split()\n",
    "        for indexCounter, word in enumerate(split):\n",
    "            try:\n",
    "                ids[fileCounter][indexCounter] = wordsList.index(word)\n",
    "            except ValueError:\n",
    "                ids[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "\n",
    "    return ids\n",
    "\n",
    "\n",
    "#train_ids = get_matrix_ids(train_pol_x, maxSeqLength)\n",
    "#test_ids = get_matrix_ids(test_pol_x, maxSeqLength)\n",
    "#np.savez('ids.npz', train_ids=train_ids, test_ids=test_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load all of train and test data\n",
    "p = np.load('train_test.npz')\n",
    "train_pol_y = p['train_pol_y']\n",
    "test_pol_y = p['test_pol_y']\n",
    "train_pol_x = p['train_pol_x']\n",
    "test_pol_x = p['test_pol_x']\n",
    "train_emo = p['train_emo']\n",
    "test_emo = p['test_emo']\n",
    "train_emo_y = p['train_emo_y']\n",
    "tests_emo_y = p['tests_emo_y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get matrix ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Matrix ids for each tweet were built using GloVe word embeddings\n",
    "# Because construction of matrix ids is computationally expensive,\n",
    "# matrix ids were saved and will simply be reloaded\n",
    "d = np.load('ids.npz')\n",
    "train_ids = d['train_ids']\n",
    "test_ids = d['test_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16840, 31)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Add extra dimension to traim_ids_emo for polarity\n",
    "\n",
    "#print train_pol_y[0]\n",
    "#print train_emo[0]\n",
    "#print train_ids[0]\n",
    "\n",
    "#pol_array = np.asarray(train_pol_y).reshape(-1,1)\n",
    "#train_ids_emo = np.append(train_ids, pol_array,axis=1)\n",
    "#print train_pol_x[0]\n",
    "#print train_ids.shape\n",
    "#print train_ids_emo.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import random\n",
    "\n",
    "# For Polarity Classifier\n",
    "def getTrainBatch(train_data, train_labels, train_ids):\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    # iterate through batch size\n",
    "    for i in range(batchSize):\n",
    "        num = randint(1, (len(train_data)-1))\n",
    "        if train_labels[num-1] == 1:\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "            \n",
    "        arr[i] = train_ids[num-1:num]\n",
    "        \n",
    "    return arr.astype(int), labels\n",
    "\n",
    "def getTestBatch(test_data, test_labels, test_ids):\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(1,(len(test_data)-1))\n",
    "        \n",
    "        if test_labels[num-1] == 1:\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "            \n",
    "        arr[i] = test_ids[num-1:num]\n",
    "        \n",
    "    return arr.astype(int), labels\n",
    "\n",
    "# For sub-emotion classifier\n",
    "def getTrainBatch_subEmo(train_data, train_labels, train_ids, batchSize, maxSeqLength):\n",
    "    labels = []\n",
    "    inds = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    # iterate through batch size\n",
    "    #for i in range(batchSize-10): \n",
    "        #num = randint(1, (len(train_data)-1))\n",
    "        \n",
    "    count = 0\n",
    "    for num in random.sample(xrange(1,(len(train_data)-1)), batchSize-10):\n",
    "        labels.append(train_labels[num-1])\n",
    "            \n",
    "        #arr[i]    \n",
    "        arr[count] = train_ids[num-1:num]\n",
    "        inds.append(num-1)\n",
    "        count +=1\n",
    "        \n",
    "    disgust = []\n",
    "    for m in range(len(train_labels)):\n",
    "        if train_labels[m][1] == 1:\n",
    "            disgust.append(m)\n",
    "    \n",
    "    #for mel in range(5):\n",
    "        #num = randint(1, (len(disgust)-1))\n",
    "    for num in random.sample(xrange(1,(len(disgust)-1)), 5):\n",
    "        ind = disgust[num]\n",
    "        labels.append(train_labels[ind])\n",
    "        arr[count] = train_ids[ind]\n",
    "        inds.append(ind)\n",
    "        count +=1\n",
    "        \n",
    "    anger = []\n",
    "    for p in range(len(train_labels)):\n",
    "        if train_labels[p][0] == 1:\n",
    "            anger.append(p)\n",
    "    \n",
    "    #for pri in range(5,10):\n",
    "        #num = randint(1, (len(anger)-1))\n",
    "    for num in random.sample(xrange(1,(len(anger)-1)), 5):\n",
    "        ind = anger[num]\n",
    "        labels.append(train_labels[ind])\n",
    "        arr[count] = train_ids[ind]\n",
    "        inds.append(ind)\n",
    "        count +=1\n",
    "    \n",
    "    return arr.astype(int), labels,inds\n",
    "\n",
    "\n",
    "def getTestBatch_subEmo(test_data, test_labels, test_ids, batchSize, maxSeqLength):\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    inds=[]\n",
    "    #for i in range(batchSize):\n",
    "        #num = randint(1,(len(test_data)-1))\n",
    "        \n",
    "    count = 0\n",
    "    for num in random.sample(xrange(1,(len(test_data)-1)), batchSize):\n",
    "        labels.append(test_labels[num-1])\n",
    "            \n",
    "        arr[count] = test_ids[num-1:num]\n",
    "        inds.append(num-1)\n",
    "        count +=1\n",
    "        \n",
    "    return arr.astype(int), labels,inds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 34, 55, 63, 88]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anger = []\n",
    "for p in range(len(train_emo_y)):\n",
    "    if train_emo_y[p][0] == 1:\n",
    "        anger.append(p)\n",
    "        \n",
    "print anger[:5]\n",
    "\n",
    "train_emo_y[55]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polarity Classifier  w/ Scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   Negative       0.79      0.85      0.82      2564\n",
      "   Positive       0.74      0.66      0.70      1647\n",
      "\n",
      "avg / total       0.77      0.78      0.77      4211\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df = 2,\n",
    "                            max_df=.5,\n",
    "                            use_idf=True,\n",
    "                            stop_words='english',\n",
    "                            sublinear_tf=True\n",
    "                             \n",
    "                            )\n",
    "train_vectors = vectorizer.fit_transform(train_pol_x)\n",
    "test_vectors = vectorizer.transform(test_pol_x)\n",
    "\n",
    "base1 = svm.LinearSVC(loss=\"hinge\")\n",
    "# base1 = svm.SVC(kernel='rbf', C=1.0, gamma=0.1)\n",
    "base1.fit(train_vectors, train_pol_y)\n",
    "predict_base1 = base1.predict(test_vectors)\n",
    "\n",
    "target_names = [\"Negative\",\"Positive\"]\n",
    "print classification_report(test_pol_y,predict_base1, target_names = target_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save as npz\n",
    "#np.savez('pol_predictions.npz', sci_svm=predict_base1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# call in npz labels\n",
    "m = np.load('pol_predictions.npz')\n",
    "predicted_svm = m['sci_svm']\n",
    "# predicted_svm\n",
    "\n",
    "#add predicted labels as [32] into test_ids \n",
    "#predict_pol = predicted_svm.reshape(-1,1)\n",
    "#test_ids_emo = np.append(test_ids, predict_pol,axis=1)\n",
    "\n",
    "#test_ids_emo.shape\n",
    "# test_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append Polarity predictions as word in tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def concat_pol(df, pol_pred, tweet):\n",
    "    new_tweet = []\n",
    "    for i in range(len(df[pol_pred])):\n",
    "        if elem == 1:\n",
    "            tw = 'positive ' + df[tweet][i]\n",
    "        else:\n",
    "            tw = 'negative ' + df[tweet][i]\n",
    "        new_tweet.append(tw)\n",
    "    return new_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Test Set\n",
    "# # Create a data frame with the tweet words & polarity prediction\n",
    "# test_df = pd.DataFrame({'test_tweet': test_pol_x, 'pol_pred': predicted_svm})\n",
    "# test_df['concat'] = concat_pol(test_df, 'pol_pred', 'test_tweet')\n",
    "\n",
    "# # Train Set\n",
    "# train_df = pd.DataFrame({'train_tweet': train_pol_x, 'pol_pred': train_pol_y.astype(int)})\n",
    "# train_df['concat'] = concat_pol(train_df, 'pol_pred', 'train_tweet')\n",
    "\n",
    "# maxSeqLength = max([len(elem.split()) for elem in data.ix[:, 4]]) + 1 #Maximum number of words in a tweet\n",
    "\n",
    "# train_ids_pol = get_matrix_ids(train_df.concat, maxSeqLength)\n",
    "# test_ids_pol = get_matrix_ids(test_df.concat, maxSeqLength)\n",
    "# np.savez('ids_pol.npz', train_ids_pol=train_ids_pol, test_ids_pol=test_ids_pol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load matrix ids with polarity prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = np.load('ids_pol.npz')\n",
    "train_ids_pol = d['train_ids_pol']\n",
    "test_ids_pol = d['test_ids_pol']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub-emotion Classifier without polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Model\n",
    "\n",
    "Changes: \n",
    "- adding forget_bias to the LSTM Cell\n",
    "- adding keep_prob\n",
    "\n",
    "Check on:\n",
    "- use of tf.nn.dynamic_rnn cell vs MultiRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Specify parameters\n",
    "\n",
    "#7/30 added 1 to increase max length to add a polarity field\n",
    "maxSeqLength = max([len(elem.split()) for elem in data.ix[:, 4]])+1 #Maximum number of words in a tweet\n",
    "batchSize = 150\n",
    "hiddenStateSize = 1\n",
    "# lstmUnits = 2\n",
    "numClasses = 6\n",
    "numDimensions = 50\n",
    "keepProb = 0.5\n",
    "learningRate = 0.01\n",
    "\n",
    "iterations = 1500\n",
    "\n",
    "# Reset graph & create placeholders\n",
    "tf.reset_default_graph()\n",
    "labels = tf.placeholder(tf.int32, [batchSize, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])\n",
    "\n",
    "##ADD NS on 8/2\n",
    "ns = tf.tile([maxSeqLength], [batchSize, ])\n",
    "\n",
    "# Lookup word vectors\n",
    "with tf.name_scope(\"Embedding_Layer\"):\n",
    "    data_vec = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "    data_vec = tf.nn.embedding_lookup(wordVectors,input_data)\n",
    "#     print \"Embedding Layer shape\", data_vec.shape\n",
    "\n",
    "# Construct RNN/LSTM cell and recurrent layer.\n",
    "#with tf.name_scope(\"Cell_RNN_Layer\"):\n",
    "    #lstmCell = tf.contrib.rnn.BasicLSTMCell(numDimensions, forget_bias=0.0)\n",
    "    #lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, input_keep_prob=keepProb, output_keep_prob=keepProb)            \n",
    "    #lstmCell = tf.contrib.rnn.MultiRNNCell([lstmCell] * hiddenStateSize)\n",
    "    #value, _ = tf.nn.dynamic_rnn(lstmCell, data_vec, dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "##NEW MULTILAYER added on 8/2\n",
    "with tf.name_scope(\"Cell_RNN_Layer\"):\n",
    "    cells=[]\n",
    "    for _ in range(hiddenStateSize):\n",
    "        lstmCell = tf.contrib.rnn.BasicLSTMCell(numDimensions, forget_bias=0.0)\n",
    "        lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, input_keep_prob=keepProb, output_keep_prob=keepProb)        \n",
    "        cells.append(lstmCell)\n",
    "        multicell = tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
    "    value, _ = tf.nn.dynamic_rnn(multicell, data_vec, sequence_length=ns, dtype=tf.float32)\n",
    "    \n",
    "#     print \"Output of RNN shape\", value.shape\n",
    "    \n",
    "with tf.name_scope(\"Output_Layer\"):\n",
    "    weight = tf.Variable(tf.random_uniform([numDimensions, numClasses], -1.0, 1.0))\n",
    "    bias = tf.Variable(tf.zeros(numClasses, tf.float32))\n",
    "    value = tf.transpose(value, [1, 0, 2])\n",
    "    last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "    multiplier = tf.matmul(last, weight)\n",
    "    prediction = tf.add(multiplier, bias)\n",
    "\n",
    "    \n",
    "with tf.name_scope(\"Prediction_Layer\"):\n",
    "    # Define correct predictions and accuracy\n",
    "    comparison = tf.argmax(prediction,1)\n",
    "    correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "\n",
    "    # Define loss & optimizer\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learningRate).minimize(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_inds = []\n",
    "train_logits = []\n",
    "train_labels = []\n",
    "for i in range(iterations):\n",
    "    # Next Batch of reviews\n",
    "    nextBatch, nextBatchLabels,train_i = getTrainBatch_subEmo(train_pol_x, train_emo_y, train_ids_pol, batchSize, maxSeqLength);\n",
    "    train_inds.append(train_i)\n",
    "    train_logs = sess.run([prediction,optimizer], {input_data: nextBatch, labels: nextBatchLabels})\n",
    "    train_logits.append(train_logs[0])\n",
    "    train_labels.append(nextBatchLabels)\n",
    "    # Write summary to Tensorboard\n",
    "    summary = sess.run(merged, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "    writer.add_summary(summary, i)\n",
    "\n",
    "#     # Save the network every 10,000 training iterations\n",
    "#     if (i % 10000 == 0 and i != 0):\n",
    "#         save_path = saver.save(sess, \"models/pretrained_lstm.ckpt\", global_step=i)\n",
    "#         print(\"saved to %s\" % save_path)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iterations = 500\n",
    "l_predictions = []\n",
    "l_labels = []\n",
    "l_logits = []\n",
    "l_inds = []\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels,test_i = getTestBatch_subEmo(test_pol_x, tests_emo_y, test_ids_pol, batchSize, maxSeqLength)\n",
    "\n",
    "    test_log,p,q= (sess.run([prediction,comparison,accuracy], {input_data: nextBatch, labels: nextBatchLabels}))\n",
    "    l_predictions.append(p)\n",
    "    l_labels.append(nextBatchLabels)\n",
    "    l_logits.append(test_log)\n",
    "    l_inds.append(test_i)\n",
    "    #print(\"Accuracy for this batch:\",q)\n",
    "\n",
    "#     print(\"Accuracy for this batch:\", (sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})) * 100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   :: anger       0.25      0.13      0.17      5263\n",
      " :: disgust       0.14      0.03      0.05      2615\n",
      "    :: fear       0.47      0.46      0.46      9920\n",
      "     :: joy       0.54      0.77      0.63     29241\n",
      " :: sadness       0.29      0.25      0.27     14441\n",
      ":: surprise       0.48      0.26      0.34     13520\n",
      "\n",
      "avg / total       0.44      0.47      0.44     75000\n",
      "\n",
      "(1, 2) 51\n",
      "(1, 0) 100\n",
      "(5, 1) 110\n",
      "(1, 3) 112\n",
      "(1, 4) 120\n",
      "(1, 5) 121\n",
      "(5, 0) 213\n",
      "(0, 5) 262\n",
      "(0, 1) 311\n",
      "(2, 1) 318\n",
      "(5, 2) 394\n",
      "(0, 4) 508\n",
      "(0, 2) 509\n",
      "(0, 3) 515\n",
      "(3, 1) 881\n",
      "(4, 1) 910\n",
      "(2, 5) 917\n",
      "(2, 0) 939\n",
      "(4, 0) 1315\n",
      "(4, 2) 1339\n",
      "(5, 4) 1371\n",
      "(2, 4) 1431\n",
      "(2, 3) 1554\n",
      "(5, 3) 1676\n",
      "(3, 0) 1992\n",
      "(4, 5) 2678\n",
      "(4, 3) 2847\n",
      "(3, 2) 3083\n",
      "(3, 5) 6034\n",
      "(3, 4) 7344\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "\n",
    "target_names = emo_bin.classes_.tolist()\n",
    "def score(preds,labels,target_names, indexes):\n",
    "    predictions = np.asarray(preds).ravel()\n",
    "    labels = np.argmax(np.asarray(labels),2).ravel()\n",
    "    indexes = np.asarray(indexes).ravel()\n",
    "    \n",
    "    print classification_report(labels,predictions,target_names=target_names)\n",
    "    \n",
    "    errors = dict()\n",
    "    examples = dict()\n",
    "    for i, p in enumerate(predictions):\n",
    "        if p != labels[i]:\n",
    "            if (p, labels[i]) not in errors:\n",
    "                errors[(p, labels[i])] = 1\n",
    "                examples[(p, labels[i])] = [indexes[i]]\n",
    "            else:\n",
    "                errors[(p, labels[i])] += 1 \n",
    "                examples[(p, labels[i])].append(indexes[i])\n",
    "                \n",
    "    return OrderedDict(sorted(errors.items(), key=itemgetter(1))), examples\n",
    "err, ex = score(l_predictions,l_labels,target_names,l_inds)\n",
    "\n",
    "# See which pairs are getting confused most often\n",
    "for key, val in err.iteritems():\n",
    "    print key, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4108 1\n",
      "22 1\n",
      "2080 1\n",
      "2085 1\n",
      "2095 1\n",
      "83 1\n",
      "157 1\n",
      "204 1\n",
      "283 1\n",
      "733 1\n",
      "2100 1\n",
      "389 1\n",
      "478 1\n",
      "538 1\n",
      "2761 1\n",
      "2767 1\n",
      "2788 1\n",
      "3539 1\n",
      "2906 1\n",
      "880 1\n",
      "2986 1\n",
      "3028 1\n",
      "3052 1\n",
      "3069 1\n",
      "1029 1\n",
      "1183 1\n",
      "3253 1\n",
      "1211 1\n",
      "3341 1\n",
      "3352 1\n",
      "3356 1\n",
      "3374 1\n",
      "3389 1\n",
      "3400 1\n",
      "1435 1\n",
      "3495 1\n",
      "3517 1\n",
      "3564 1\n",
      "3604 1\n",
      "3611 1\n",
      "3614 1\n",
      "1684 1\n",
      "2923 1\n",
      "1817 1\n",
      "1838 1\n",
      "3926 1\n",
      "1919 1\n",
      "3976 1\n",
      "3993 1\n",
      "4050 1\n",
      "2014 1\n",
      "43 2\n",
      "77 2\n",
      "243 2\n",
      "378 2\n",
      "442 2\n",
      "444 2\n",
      "526 2\n",
      "2608 2\n",
      "2609 2\n",
      "2697 2\n",
      "776 2\n",
      "781 2\n",
      "2831 2\n",
      "794 2\n",
      "1842 2\n",
      "896 2\n",
      "2951 2\n",
      "935 2\n",
      "939 2\n",
      "946 2\n",
      "976 2\n",
      "3072 2\n",
      "1050 2\n",
      "3304 2\n",
      "3282 2\n",
      "1366 2\n",
      "1461 2\n",
      "3514 2\n",
      "3556 2\n",
      "1557 2\n",
      "3647 2\n",
      "1621 2\n",
      "3765 2\n",
      "4056 2\n",
      "11 3\n",
      "2071 3\n",
      "4147 3\n",
      "4190 3\n",
      "158 3\n",
      "2674 3\n",
      "675 3\n",
      "2814 3\n",
      "804 3\n",
      "2901 3\n",
      "3037 3\n",
      "1123 3\n",
      "3223 3\n",
      "3224 3\n",
      "1284 3\n",
      "3358 3\n",
      "1545 3\n",
      "1560 3\n",
      "1610 3\n",
      "1633 3\n",
      "3879 3\n",
      "1872 3\n",
      "1881 3\n",
      "3934 3\n",
      "3996 3\n",
      "17 4\n",
      "128 4\n",
      "152 4\n",
      "2312 4\n",
      "347 4\n",
      "450 4\n",
      "475 4\n",
      "497 4\n",
      "638 4\n",
      "2713 4\n",
      "2842 4\n",
      "2907 4\n",
      "1217 4\n",
      "3273 4\n",
      "931 4\n",
      "3568 4\n",
      "3804 4\n",
      "1853 4\n",
      "3923 4\n",
      "3935 4\n",
      "3952 4\n",
      "1952 4\n",
      "4078 4\n",
      "4111 5\n",
      "4149 5\n",
      "111 5\n",
      "424 5\n",
      "3088 5\n",
      "3593 5\n",
      "3138 5\n",
      "3159 5\n",
      "1232 5\n",
      "3393 5\n",
      "1404 5\n",
      "1425 5\n",
      "1577 5\n",
      "1826 5\n",
      "1837 5\n",
      "357 6\n",
      "500 6\n",
      "2560 6\n",
      "2641 6\n",
      "694 6\n",
      "999 6\n",
      "3063 6\n",
      "1042 6\n",
      "1347 6\n",
      "1604 6\n",
      "1845 6\n",
      "3933 6\n",
      "4070 6\n",
      "2067 7\n",
      "2320 7\n",
      "336 7\n",
      "2420 7\n",
      "2471 7\n",
      "2487 7\n",
      "2514 7\n",
      "2524 7\n",
      "2525 7\n",
      "604 7\n",
      "2669 7\n",
      "2979 7\n",
      "3040 7\n",
      "3062 7\n",
      "3261 7\n",
      "1301 7\n",
      "3407 7\n",
      "1369 7\n",
      "3771 7\n",
      "1736 7\n",
      "1745 7\n",
      "1824 7\n",
      "1909 7\n",
      "2119 8\n",
      "101 8\n",
      "2256 8\n",
      "413 8\n",
      "2501 8\n",
      "471 8\n",
      "553 8\n",
      "3506 8\n",
      "644 8\n",
      "703 8\n",
      "789 8\n",
      "2938 8\n",
      "934 8\n",
      "3099 8\n",
      "1070 8\n",
      "3246 8\n",
      "1241 8\n",
      "2601 8\n",
      "3453 8\n",
      "3456 8\n",
      "3543 8\n",
      "1590 8\n",
      "1724 8\n",
      "3775 8\n",
      "1800 8\n",
      "4063 8\n",
      "78 9\n",
      "150 9\n",
      "171 9\n",
      "228 9\n",
      "242 9\n",
      "246 9\n",
      "2340 9\n",
      "327 9\n",
      "410 9\n",
      "2687 9\n",
      "2857 9\n",
      "487 9\n",
      "923 9\n",
      "3067 9\n",
      "1268 9\n",
      "1334 9\n",
      "1361 9\n",
      "1458 9\n",
      "1481 9\n",
      "1575 9\n",
      "1592 9\n",
      "1721 9\n",
      "1729 9\n",
      "3844 9\n",
      "1869 9\n",
      "1958 9\n",
      "4035 9\n",
      "1041 10\n",
      "2173 10\n",
      "2370 10\n",
      "352 10\n",
      "2434 10\n",
      "455 10\n",
      "688 10\n",
      "2795 10\n",
      "922 10\n",
      "1105 10\n",
      "1343 10\n",
      "1388 10\n",
      "1412 10\n",
      "3655 10\n",
      "3768 10\n",
      "3800 10\n",
      "1924 10\n",
      "4027 10\n",
      "2078 11\n",
      "52 11\n",
      "2161 11\n",
      "580 11\n",
      "2679 11\n",
      "2694 11\n",
      "697 11\n",
      "757 11\n",
      "824 11\n",
      "1002 11\n",
      "3242 11\n",
      "3252 11\n",
      "3384 11\n",
      "1726 11\n",
      "3859 11\n",
      "31 12\n",
      "4158 12\n",
      "2132 12\n",
      "156 12\n",
      "332 12\n",
      "449 12\n",
      "525 12\n",
      "2595 12\n",
      "635 12\n",
      "2883 12\n",
      "965 12\n",
      "3089 12\n",
      "1155 12\n",
      "1234 12\n",
      "1289 12\n",
      "1367 12\n",
      "1386 12\n",
      "3811 12\n",
      "2172 13\n",
      "147 13\n",
      "306 13\n",
      "84 13\n",
      "2612 13\n",
      "586 13\n",
      "1466 13\n",
      "611 13\n",
      "2755 13\n",
      "2781 13\n",
      "734 13\n",
      "918 13\n",
      "962 13\n",
      "3021 13\n",
      "3105 13\n",
      "1354 13\n",
      "1415 13\n",
      "3559 13\n",
      "1778 13\n",
      "1981 13\n",
      "4041 13\n",
      "4125 14\n",
      "3436 14\n",
      "2313 14\n",
      "2367 14\n",
      "2440 14\n",
      "484 14\n",
      "2572 14\n",
      "2787 14\n",
      "2820 14\n",
      "2858 14\n",
      "829 14\n",
      "897 14\n",
      "926 14\n",
      "1203 14\n",
      "3369 14\n",
      "2366 14\n",
      "2143 15\n",
      "2125 15\n",
      "2561 15\n",
      "515 15\n",
      "672 15\n",
      "807 15\n",
      "1037 15\n",
      "3422 15\n",
      "3577 15\n",
      "1603 15\n",
      "1627 15\n",
      "1979 15\n",
      "3850 15\n",
      "3409 15\n",
      "2277 16\n",
      "2348 16\n",
      "301 16\n",
      "2416 16\n",
      "3164 16\n",
      "1313 16\n",
      "1548 16\n",
      "3737 16\n",
      "1883 16\n",
      "3966 16\n",
      "2128 17\n",
      "2403 17\n",
      "1223 17\n",
      "3335 17\n",
      "3430 17\n",
      "3601 17\n",
      "3618 17\n",
      "1605 17\n",
      "3724 17\n",
      "4061 17\n",
      "2109 18\n",
      "3784 18\n",
      "433 18\n",
      "511 18\n",
      "2916 18\n",
      "1139 18\n",
      "3527 18\n",
      "269 19\n",
      "2581 19\n",
      "2584 19\n",
      "2949 19\n",
      "970 19\n",
      "1080 19\n",
      "172 20\n",
      "578 20\n",
      "2763 20\n",
      "761 20\n",
      "1936 20\n",
      "4051 20\n",
      "56 21\n",
      "318 21\n",
      "2405 21\n",
      "2682 21\n",
      "2746 21\n",
      "1931 21\n",
      "488 22\n",
      "3144 22\n",
      "1514 22\n",
      "1714 22\n",
      "587 23\n",
      "3619 23\n",
      "3890 24\n",
      "1896 24\n",
      "4202 25\n",
      "879 27\n",
      "1900 32\n"
     ]
    }
   ],
   "source": [
    "biggest_screw_ups = dict()\n",
    "for elem in ex[(3, 2)]:\n",
    "    if elem not in biggest_screw_ups:\n",
    "        biggest_screw_ups[elem] = 1\n",
    "    else:\n",
    "        biggest_screw_ups[elem] += 1\n",
    "d = OrderedDict(sorted(biggest_screw_ups.items(), key=itemgetter(1)))\n",
    "for key, value in d.iteritems():\n",
    "    print key, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion: [0 0 1 0 0 0]\n",
      "hot5s competing at the petersen event center tomorrow come out and support them this\n",
      "\n",
      "Emotion: [0 0 1 0 0 0]\n",
      "factor is back tomorrow\n",
      "\n",
      "Emotion: [0 0 1 0 0 0]\n",
      "be of good courage and not for jesus christ is right now working on your behalf and his zeal for you will perform thisisaiah 97 fb\n",
      "\n",
      "Emotion: [0 0 1 0 0 0]\n",
      "ive a funny feeling the i painstakingly planned for tomorrow is going to fail too many it issues today dayoffruined\n",
      "\n",
      "Emotion: [0 0 1 0 0 0]\n",
      "darleeen23 everytime i come home from work at nite i sprint to the house\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Indexes where prediction was 3, actual was 4\n",
    "# mixups = [2, 28, 2545, 1471, 2790]\n",
    "# Indexes where prediction was 3, actual was 5\n",
    "# mixups = [3236, 390, 1852, 1812, 3854]\n",
    "# Indexes where prediction was 3, actual was 2\n",
    "mixups = [1900, 879, 4202, 1896, 3890]\n",
    "for elem in mixups:\n",
    "    print \"Emotion:\", tests_emo_y[elem]\n",
    "    print test_pol_x[elem] + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just got back from my last history class until next semestre\n",
      "[0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print test_pol_x[2]\n",
    "print tests_emo_y[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
