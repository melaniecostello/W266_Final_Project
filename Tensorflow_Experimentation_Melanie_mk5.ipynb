{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import HTMLParser as htm\n",
    "import string\n",
    "import re\n",
    "import time\n",
    "\n",
    "# SK-learn library for splitting data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Positive</th>\n",
       "      <th>escape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138881940341260288:</td>\n",
       "      <td>I got a surprise for all you bitches...pull th...</td>\n",
       "      <td>:: surprise</td>\n",
       "      <td>0</td>\n",
       "      <td>I got a surprise for all you bitches...pull th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>144479819843911683:</td>\n",
       "      <td>If I was a thief.. The first thing I would ste...</td>\n",
       "      <td>:: joy</td>\n",
       "      <td>1</td>\n",
       "      <td>If I was a thief.. The first thing I would ste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>139110849120972800:</td>\n",
       "      <td>\"&amp;quot;@RevRunWisdom: not afraid of tomorrow, ...</td>\n",
       "      <td>:: fear</td>\n",
       "      <td>0</td>\n",
       "      <td>\"\"@RevRunWisdom: not afraid of tomorrow, for I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>141532076791971840:</td>\n",
       "      <td>\"Extreme can neither fight nor fly.&amp;#xA;-- Wil...</td>\n",
       "      <td>:: fear</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Extreme can neither fight nor fly.\\n-- Willia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>145353048817012736:</td>\n",
       "      <td>Thinks that @melbahughes had a great 50th birt...</td>\n",
       "      <td>:: surprise</td>\n",
       "      <td>0</td>\n",
       "      <td>Thinks that @melbahughes had a great 50th birt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Id                                              Tweet  \\\n",
       "0  138881940341260288:  I got a surprise for all you bitches...pull th...   \n",
       "1  144479819843911683:  If I was a thief.. The first thing I would ste...   \n",
       "2  139110849120972800:  \"&quot;@RevRunWisdom: not afraid of tomorrow, ...   \n",
       "3  141532076791971840:  \"Extreme can neither fight nor fly.&#xA;-- Wil...   \n",
       "4  145353048817012736:  Thinks that @melbahughes had a great 50th birt...   \n",
       "\n",
       "       Emotion  Positive                                             escape  \n",
       "0  :: surprise         0  I got a surprise for all you bitches...pull th...  \n",
       "1       :: joy         1  If I was a thief.. The first thing I would ste...  \n",
       "2      :: fear         0  \"\"@RevRunWisdom: not afraid of tomorrow, for I...  \n",
       "3      :: fear         0  \"Extreme can neither fight nor fly.\\n-- Willia...  \n",
       "4  :: surprise         0  Thinks that @melbahughes had a great 50th birt...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"tweet_data_1.csv\",sep='\\t',quoting=3)\n",
    "data[\"escape\"] = data.apply(lambda row: htm.HTMLParser().unescape(row[1].decode(\"utf-8\")),axis=1)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    \"\"\"Converts to lowercase, strips out punctuation,\n",
    "    removes excess whitespace within a string & leading & trailing whitespace\"\"\"\n",
    "    new_list = []\n",
    "    table = string.maketrans(\"\",\"\")\n",
    "    for elem in data:\n",
    "        elem = \"\".join(i for i in elem if ord(i)<128)\n",
    "        elem = str(elem)        \n",
    "        elem = elem.lower()\n",
    "        elem = elem.translate(table, string.punctuation)\n",
    "        elem = re.sub(' +',' ', elem)\n",
    "        elem = elem.strip()\n",
    "        \n",
    "        new_list.append(elem)\n",
    "    return new_list\n",
    "\n",
    "#train_pol_x = process_data(train_pol_x)\n",
    "#test_pol_x = process_data(test_pol_x)\n",
    "\n",
    "#Clean entire data set at once\n",
    "data.escape = process_data(data.escape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train & test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train and test data frames\n",
    "train, test = train_test_split(data, test_size = 0.2)\n",
    "\n",
    "# Train and test target labels for polarity\n",
    "train_pol_y = train.ix[:,3].tolist()\n",
    "test_pol_y = test.ix[:,3].tolist()\n",
    "\n",
    "# Binarize labels for sub-emotion classifier\n",
    "train_emo = train.ix[:,2].tolist()\n",
    "test_emo = test.ix[:,2].tolist()\n",
    "emo_bin = preprocessing.LabelBinarizer()\n",
    "\n",
    "# Labels for sub-emotion classifier\n",
    "train_emo_y = emo_bin.fit_transform(train_emo)\n",
    "tests_emo_y = emo_bin.transform(test_emo)\n",
    "\n",
    "# Train and test inputs\n",
    "train_pol_x = train.ix[:, 4].tolist()\n",
    "test_pol_x = test.ix[:, 4].tolist()\n",
    "\n",
    "\n",
    "#save data to recall later\n",
    "#np.savez('train_test.npz', train_pol_y=train_pol_y, test_pol_y=test_pol_y,train_pol_x=train_pol_x,\\\n",
    "        #test_pol_x=test_pol_x, train_emo=train_emo,test_emo=test_emo,train_emo_y=train_emo_y,\\\n",
    "        #tests_emo_y=tests_emo_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull in GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the word list!\n",
      "Loaded the word vectors!\n"
     ]
    }
   ],
   "source": [
    "# Pull in word list & vectors\n",
    "wordsList = np.load('wordsList.npy')\n",
    "print('Loaded the word list!')\n",
    "wordsList = wordsList.tolist() #Originally loaded as numpy array\n",
    "wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "wordVectors = np.load('wordVectors.npy')\n",
    "print ('Loaded the word vectors!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxSeqLength = max([len(elem.split()) for elem in data.ix[:, 4]]) #Maximum number of words in a tweet\n",
    "\n",
    "\n",
    "def get_matrix_ids(data, maxSeqLength):\n",
    "    numFiles = len(data)\n",
    "    ids = np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
    "\n",
    "    for fileCounter, tweet in enumerate(data):\n",
    "        start = time.time()\n",
    "        split = tweet.split()\n",
    "        for indexCounter, word in enumerate(split):\n",
    "            try:\n",
    "                ids[fileCounter][indexCounter] = wordsList.index(word)\n",
    "            except ValueError:\n",
    "                ids[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "\n",
    "    return ids\n",
    "\n",
    "\n",
    "#train_ids = get_matrix_ids(train_pol_x, maxSeqLength)\n",
    "#test_ids = get_matrix_ids(test_pol_x, maxSeqLength)\n",
    "#np.savez('ids.npz', train_ids=train_ids, test_ids=test_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load all of train and test data\n",
    "p = np.load('train_test.npz')\n",
    "train_pol_y = p['train_pol_y']\n",
    "test_pol_y = p['test_pol_y']\n",
    "train_pol_x = p['train_pol_x']\n",
    "test_pol_x = p['test_pol_x']\n",
    "train_emo = p['train_emo']\n",
    "test_emo = p['test_emo']\n",
    "train_emo_y = p['train_emo_y']\n",
    "tests_emo_y = p['tests_emo_y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get matrix ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Matrix ids for each tweet were built using GloVe word embeddings\n",
    "# Because construction of matrix ids is computationally expensive,\n",
    "# matrix ids were saved and will simply be reloaded\n",
    "d = np.load('ids.npz')\n",
    "train_ids = d['train_ids']\n",
    "test_ids = d['test_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16840, 31)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Add extra dimension to traim_ids_emo for polarity\n",
    "\n",
    "#print train_pol_y[0]\n",
    "#print train_emo[0]\n",
    "#print train_ids[0]\n",
    "\n",
    "#pol_array = np.asarray(train_pol_y).reshape(-1,1)\n",
    "#train_ids_emo = np.append(train_ids, pol_array,axis=1)\n",
    "#print train_pol_x[0]\n",
    "#print train_ids.shape\n",
    "#print train_ids_emo.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import random\n",
    "\n",
    "# For Polarity Classifier\n",
    "def getTrainBatch(train_data, train_labels, train_ids):\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    # iterate through batch size\n",
    "    for i in range(batchSize):\n",
    "        num = randint(1, (len(train_data)-1))\n",
    "        if train_labels[num-1] == 1:\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "            \n",
    "        arr[i] = train_ids[num-1:num]\n",
    "        \n",
    "    return arr.astype(int), labels\n",
    "\n",
    "def getTestBatch(test_data, test_labels, test_ids):\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(1,(len(test_data)-1))\n",
    "        \n",
    "        if test_labels[num-1] == 1:\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "            \n",
    "        arr[i] = test_ids[num-1:num]\n",
    "        \n",
    "    return arr.astype(int), labels\n",
    "\n",
    "# For sub-emotion classifier\n",
    "def getTrainBatch_subEmo(train_data, train_labels, train_ids, batchSize, maxSeqLength):\n",
    "    labels = []\n",
    "    inds = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    # iterate through batch size\n",
    "    #for i in range(batchSize-10): \n",
    "        #num = randint(1, (len(train_data)-1))\n",
    "        \n",
    "    count = 0\n",
    "    for num in random.sample(xrange(1,(len(train_data)-1)), batchSize-10):\n",
    "        labels.append(train_labels[num-1])\n",
    "            \n",
    "        #arr[i]    \n",
    "        arr[count] = train_ids[num-1:num]\n",
    "        inds.append(num-1)\n",
    "        count +=1\n",
    "        \n",
    "    disgust = []\n",
    "    for m in range(len(train_labels)):\n",
    "        if train_labels[m][1] == 1:\n",
    "            disgust.append(m)\n",
    "    \n",
    "    #for mel in range(5):\n",
    "        #num = randint(1, (len(disgust)-1))\n",
    "    for num in random.sample(xrange(1,(len(disgust)-1)), 5):\n",
    "        ind = disgust[num]\n",
    "        labels.append(train_labels[ind])\n",
    "        arr[count] = train_ids[ind]\n",
    "        inds.append(ind)\n",
    "        count +=1\n",
    "        \n",
    "    anger = []\n",
    "    for p in range(len(train_labels)):\n",
    "        if train_labels[p][0] == 1:\n",
    "            anger.append(p)\n",
    "    \n",
    "    #for pri in range(5,10):\n",
    "        #num = randint(1, (len(anger)-1))\n",
    "    for num in random.sample(xrange(1,(len(anger)-1)), 5):\n",
    "        ind = anger[num]\n",
    "        labels.append(train_labels[ind])\n",
    "        arr[count] = train_ids[ind]\n",
    "        inds.append(ind)\n",
    "        count +=1\n",
    "    \n",
    "    return arr.astype(int), labels,inds\n",
    "\n",
    "\n",
    "def getTestBatch_subEmo(test_data, test_labels, test_ids, batchSize, maxSeqLength):\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    inds=[]\n",
    "    #for i in range(batchSize):\n",
    "        #num = randint(1,(len(test_data)-1))\n",
    "        \n",
    "    count = 0\n",
    "    for num in random.sample(xrange(1,(len(test_data)-1)), batchSize):\n",
    "        labels.append(test_labels[num-1])\n",
    "            \n",
    "        arr[count] = test_ids[num-1:num]\n",
    "        inds.append(num-1)\n",
    "        count +=1\n",
    "        \n",
    "    return arr.astype(int), labels,inds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 34, 55, 63, 88]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anger = []\n",
    "for p in range(len(train_emo_y)):\n",
    "    if train_emo_y[p][0] == 1:\n",
    "        anger.append(p)\n",
    "        \n",
    "print anger[:5]\n",
    "\n",
    "train_emo_y[55]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polarity Classifier  w/ Scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   Negative       0.79      0.85      0.82      2564\n",
      "   Positive       0.74      0.66      0.70      1647\n",
      "\n",
      "avg / total       0.77      0.78      0.77      4211\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df = 2,\n",
    "                            max_df=.5,\n",
    "                            use_idf=True,\n",
    "                            stop_words='english',\n",
    "                            sublinear_tf=True\n",
    "                             \n",
    "                            )\n",
    "train_vectors = vectorizer.fit_transform(train_pol_x)\n",
    "test_vectors = vectorizer.transform(test_pol_x)\n",
    "\n",
    "base1 = svm.LinearSVC(loss=\"hinge\")#svm.SVC(kernel='linear')\n",
    "base1.fit(train_vectors, train_pol_y)\n",
    "predict_base1 = base1.predict(test_vectors)\n",
    "\n",
    "target_names = [\"Negative\",\"Positive\"]\n",
    "print classification_report(test_pol_y,predict_base1, target_names = target_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save as npz\n",
    "#np.savez('pol_predictions.npz', sci_svm=predict_base1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# call in npz labels\n",
    "m = np.load('pol_predictions.npz')\n",
    "predicted_svm = m['sci_svm']\n",
    "# predicted_svm\n",
    "\n",
    "#add predicted labels as [32] into test_ids \n",
    "#predict_pol = predicted_svm.reshape(-1,1)\n",
    "#test_ids_emo = np.append(test_ids, predict_pol,axis=1)\n",
    "\n",
    "#test_ids_emo.shape\n",
    "# test_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append Polarity predictions as word in tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def concat_pol(df, pol_pred, tweet):\n",
    "    new_tweet = []\n",
    "    for i in range(len(df[pol_pred])):\n",
    "        if elem == 1:\n",
    "            tw = 'positive ' + df[tweet][i]\n",
    "        else:\n",
    "            tw = 'negative ' + df[tweet][i]\n",
    "        new_tweet.append(tw)\n",
    "    return new_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pol_pred</th>\n",
       "      <th>test_tweet</th>\n",
       "      <th>concat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>first case done chambers have now found more w...</td>\n",
       "      <td>negative first case done chambers have now fou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>robertlemke so rocking flow3 yeah have fun btw...</td>\n",
       "      <td>negative robertlemke so rocking flow3 yeah hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>just got back from my last history class until...</td>\n",
       "      <td>negative just got back from my last history cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>abi a4akir w rasi y3wrni w abi anam w abi atra...</td>\n",
       "      <td>negative abi a4akir w rasi y3wrni w abi anam w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>hearnesie71 sleep overs tuesday after hockey a...</td>\n",
       "      <td>negative hearnesie71 sleep overs tuesday after...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pol_pred                                         test_tweet  \\\n",
       "0         1  first case done chambers have now found more w...   \n",
       "1         0  robertlemke so rocking flow3 yeah have fun btw...   \n",
       "2         0  just got back from my last history class until...   \n",
       "3         0  abi a4akir w rasi y3wrni w abi anam w abi atra...   \n",
       "4         0  hearnesie71 sleep overs tuesday after hockey a...   \n",
       "\n",
       "                                              concat  \n",
       "0  negative first case done chambers have now fou...  \n",
       "1  negative robertlemke so rocking flow3 yeah hav...  \n",
       "2  negative just got back from my last history cl...  \n",
       "3  negative abi a4akir w rasi y3wrni w abi anam w...  \n",
       "4  negative hearnesie71 sleep overs tuesday after...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Test Set\n",
    "# # Create a data frame with the tweet words & polarity prediction\n",
    "# test_df = pd.DataFrame({'test_tweet': test_pol_x, 'pol_pred': predicted_svm})\n",
    "# test_df['concat'] = concat_pol(test_df, 'pol_pred', 'test_tweet')\n",
    "\n",
    "# # Train Set\n",
    "# train_df = pd.DataFrame({'train_tweet': train_pol_x, 'pol_pred': train_pol_y.astype(int)})\n",
    "# train_df['concat'] = concat_pol(train_df, 'pol_pred', 'train_tweet')\n",
    "\n",
    "# maxSeqLength = max([len(elem.split()) for elem in data.ix[:, 4]]) + 1 #Maximum number of words in a tweet\n",
    "\n",
    "# train_ids_pol = get_matrix_ids(train_df.concat, maxSeqLength)\n",
    "# test_ids_pol = get_matrix_ids(test_df.concat, maxSeqLength)\n",
    "# np.savez('ids_pol.npz', train_ids_pol=train_ids_pol, test_ids_pol=test_ids_pol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load matrix ids with polarity prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = np.load('ids_pol.npz')\n",
    "train_ids_pol = d['train_ids_pol']\n",
    "test_ids_pol = d['test_ids_pol']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub-emotion Classifier without polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Model\n",
    "\n",
    "Changes: \n",
    "- adding forget_bias to the LSTM Cell\n",
    "- adding keep_prob\n",
    "\n",
    "Check on:\n",
    "- use of tf.nn.dynamic_rnn cell vs MultiRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Specify parameters\n",
    "\n",
    "#7/30 added 1 to increase max length to add a polarity field\n",
    "maxSeqLength = max([len(elem.split()) for elem in data.ix[:, 4]])+1 #Maximum number of words in a tweet\n",
    "batchSize = 150\n",
    "hiddenStateSize = 1\n",
    "# lstmUnits = 2\n",
    "numClasses = 6\n",
    "numDimensions = 50\n",
    "keepProb = 0.5\n",
    "learningRate = 0.001\n",
    "\n",
    "iterations = 1500\n",
    "\n",
    "# Reset graph & create placeholders\n",
    "tf.reset_default_graph()\n",
    "labels = tf.placeholder(tf.int32, [batchSize, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])\n",
    "\n",
    "##ADD NS on 8/2\n",
    "ns = tf.tile([maxSeqLength], [batchSize, ])\n",
    "\n",
    "# Lookup word vectors\n",
    "with tf.name_scope(\"Embedding_Layer\"):\n",
    "    data_vec = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "    data_vec = tf.nn.embedding_lookup(wordVectors,input_data)\n",
    "#     print \"Embedding Layer shape\", data_vec.shape\n",
    "\n",
    "# Construct RNN/LSTM cell and recurrent layer.\n",
    "#with tf.name_scope(\"Cell_RNN_Layer\"):\n",
    "    #lstmCell = tf.contrib.rnn.BasicLSTMCell(numDimensions, forget_bias=0.0)\n",
    "    #lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, input_keep_prob=keepProb, output_keep_prob=keepProb)            \n",
    "    #lstmCell = tf.contrib.rnn.MultiRNNCell([lstmCell] * hiddenStateSize)\n",
    "    #value, _ = tf.nn.dynamic_rnn(lstmCell, data_vec, dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "##NEW MULTILAYER added on 8/2\n",
    "with tf.name_scope(\"Cell_RNN_Layer\"):\n",
    "    cells=[]\n",
    "    for _ in range(hiddenStateSize):\n",
    "        lstmCell = tf.contrib.rnn.BasicLSTMCell(numDimensions, forget_bias=0.0)\n",
    "        lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, input_keep_prob=keepProb, output_keep_prob=keepProb)        \n",
    "        cells.append(lstmCell)\n",
    "        multicell = tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
    "    value, _ = tf.nn.dynamic_rnn(multicell, data_vec, sequence_length=ns, dtype=tf.float32)\n",
    "    \n",
    "#     print \"Output of RNN shape\", value.shape\n",
    "    \n",
    "with tf.name_scope(\"Output_Layer\"):\n",
    "    weight = tf.Variable(tf.random_uniform([numDimensions, numClasses], -1.0, 1.0))\n",
    "    bias = tf.Variable(tf.zeros(numClasses, tf.float32))\n",
    "    value = tf.transpose(value, [1, 0, 2])\n",
    "    last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "    multiplier = tf.matmul(last, weight)\n",
    "    prediction = tf.add(multiplier, bias)\n",
    "\n",
    "    \n",
    "with tf.name_scope(\"Prediction_Layer\"):\n",
    "    # Define correct predictions and accuracy\n",
    "    comparison = tf.argmax(prediction,1)\n",
    "    correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "\n",
    "    # Define loss & optimizer\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learningRate).minimize(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_inds = []\n",
    "train_logits = []\n",
    "train_labels = []\n",
    "for i in range(iterations):\n",
    "    # Next Batch of reviews\n",
    "    nextBatch, nextBatchLabels,train_i = getTrainBatch_subEmo(train_pol_x, train_emo_y, train_ids_pol, batchSize, maxSeqLength);\n",
    "    train_inds.append(train_i)\n",
    "    train_logs = sess.run([prediction,optimizer], {input_data: nextBatch, labels: nextBatchLabels})\n",
    "    train_logits.append(train_logs[0])\n",
    "    train_labels.append(nextBatchLabels)\n",
    "    # Write summary to Tensorboard\n",
    "    summary = sess.run(merged, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "    writer.add_summary(summary, i)\n",
    "\n",
    "#     # Save the network every 10,000 training iterations\n",
    "#     if (i % 10000 == 0 and i != 0):\n",
    "#         save_path = saver.save(sess, \"models/pretrained_lstm.ckpt\", global_step=i)\n",
    "#         print(\"saved to %s\" % save_path)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iterations = 500\n",
    "l_predictions = []\n",
    "l_labels = []\n",
    "l_logits = []\n",
    "l_inds = []\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels,test_i = getTestBatch_subEmo(test_pol_x, tests_emo_y, test_ids_pol, batchSize, maxSeqLength)\n",
    "\n",
    "    test_log,p,q= (sess.run([prediction,comparison,accuracy], {input_data: nextBatch, labels: nextBatchLabels}))\n",
    "    l_predictions.append(p)\n",
    "    l_labels.append(nextBatchLabels)\n",
    "    l_logits.append(test_log)\n",
    "    l_inds.append(test_i)\n",
    "    #print(\"Accuracy for this batch:\",q)\n",
    "\n",
    "#     print(\"Accuracy for this batch:\", (sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})) * 100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   :: anger       0.17      0.08      0.11      5117\n",
      " :: disgust       0.13      0.01      0.02      2653\n",
      "    :: fear       0.53      0.34      0.41      9789\n",
      "     :: joy       0.55      0.76      0.63     29432\n",
      " :: sadness       0.27      0.23      0.25     14311\n",
      ":: surprise       0.41      0.39      0.40     13698\n",
      "\n",
      "avg / total       0.42      0.46      0.43     75000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_names = emo_bin.classes_.tolist()\n",
    "def score(preds,labels,target_names):\n",
    "    predictions = np.asarray(preds).ravel()\n",
    "    labels = np.argmax(np.asarray(labels),2).ravel()\n",
    "    \n",
    "    print classification_report(labels,predictions,target_names=target_names)\n",
    "    \n",
    "score(l_predictions,l_labels,target_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
