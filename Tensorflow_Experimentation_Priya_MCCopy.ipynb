{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import HTMLParser as htm\n",
    "import string\n",
    "import re\n",
    "import time\n",
    "\n",
    "# SK-learn library for splitting data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in and preprocess/clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Positive</th>\n",
       "      <th>escape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138881940341260288:</td>\n",
       "      <td>I got a surprise for all you bitches...pull th...</td>\n",
       "      <td>:: surprise</td>\n",
       "      <td>0</td>\n",
       "      <td>I got a surprise for all you bitches...pull th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>144479819843911683:</td>\n",
       "      <td>If I was a thief.. The first thing I would ste...</td>\n",
       "      <td>:: joy</td>\n",
       "      <td>1</td>\n",
       "      <td>If I was a thief.. The first thing I would ste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>139110849120972800:</td>\n",
       "      <td>\"&amp;quot;@RevRunWisdom: not afraid of tomorrow, ...</td>\n",
       "      <td>:: fear</td>\n",
       "      <td>0</td>\n",
       "      <td>\"\"@RevRunWisdom: not afraid of tomorrow, for I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>141532076791971840:</td>\n",
       "      <td>\"Extreme can neither fight nor fly.&amp;#xA;-- Wil...</td>\n",
       "      <td>:: fear</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Extreme can neither fight nor fly.\\n-- Willia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>145353048817012736:</td>\n",
       "      <td>Thinks that @melbahughes had a great 50th birt...</td>\n",
       "      <td>:: surprise</td>\n",
       "      <td>0</td>\n",
       "      <td>Thinks that @melbahughes had a great 50th birt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Id                                              Tweet  \\\n",
       "0  138881940341260288:  I got a surprise for all you bitches...pull th...   \n",
       "1  144479819843911683:  If I was a thief.. The first thing I would ste...   \n",
       "2  139110849120972800:  \"&quot;@RevRunWisdom: not afraid of tomorrow, ...   \n",
       "3  141532076791971840:  \"Extreme can neither fight nor fly.&#xA;-- Wil...   \n",
       "4  145353048817012736:  Thinks that @melbahughes had a great 50th birt...   \n",
       "\n",
       "       Emotion  Positive                                             escape  \n",
       "0  :: surprise         0  I got a surprise for all you bitches...pull th...  \n",
       "1       :: joy         1  If I was a thief.. The first thing I would ste...  \n",
       "2      :: fear         0  \"\"@RevRunWisdom: not afraid of tomorrow, for I...  \n",
       "3      :: fear         0  \"Extreme can neither fight nor fly.\\n-- Willia...  \n",
       "4  :: surprise         0  Thinks that @melbahughes had a great 50th birt...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"tweet_data_1.csv\",sep='\\t',quoting=3)\n",
    "data[\"escape\"] = data.apply(lambda row: htm.HTMLParser().unescape(row[1].decode(\"utf-8\")),axis=1)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Positive</th>\n",
       "      <th>escape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138881940341260288:</td>\n",
       "      <td>I got a surprise for all you bitches...pull th...</td>\n",
       "      <td>:: surprise</td>\n",
       "      <td>0</td>\n",
       "      <td>i got a surprise for all you bitchespull theri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>144479819843911683:</td>\n",
       "      <td>If I was a thief.. The first thing I would ste...</td>\n",
       "      <td>:: joy</td>\n",
       "      <td>1</td>\n",
       "      <td>if i was a thief the first thing i would steal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>139110849120972800:</td>\n",
       "      <td>\"&amp;quot;@RevRunWisdom: not afraid of tomorrow, ...</td>\n",
       "      <td>:: fear</td>\n",
       "      <td>0</td>\n",
       "      <td>revrunwisdom not afraid of tomorrow for i have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>141532076791971840:</td>\n",
       "      <td>\"Extreme can neither fight nor fly.&amp;#xA;-- Wil...</td>\n",
       "      <td>:: fear</td>\n",
       "      <td>0</td>\n",
       "      <td>extreme can neither fight nor fly\\n william sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>145353048817012736:</td>\n",
       "      <td>Thinks that @melbahughes had a great 50th birt...</td>\n",
       "      <td>:: surprise</td>\n",
       "      <td>0</td>\n",
       "      <td>thinks that melbahughes had a great 50th birth...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Id                                              Tweet  \\\n",
       "0  138881940341260288:  I got a surprise for all you bitches...pull th...   \n",
       "1  144479819843911683:  If I was a thief.. The first thing I would ste...   \n",
       "2  139110849120972800:  \"&quot;@RevRunWisdom: not afraid of tomorrow, ...   \n",
       "3  141532076791971840:  \"Extreme can neither fight nor fly.&#xA;-- Wil...   \n",
       "4  145353048817012736:  Thinks that @melbahughes had a great 50th birt...   \n",
       "\n",
       "       Emotion  Positive                                             escape  \n",
       "0  :: surprise         0  i got a surprise for all you bitchespull theri...  \n",
       "1       :: joy         1  if i was a thief the first thing i would steal...  \n",
       "2      :: fear         0  revrunwisdom not afraid of tomorrow for i have...  \n",
       "3      :: fear         0  extreme can neither fight nor fly\\n william sh...  \n",
       "4  :: surprise         0  thinks that melbahughes had a great 50th birth...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_data(data):\n",
    "    \"\"\"Converts to lowercase, strips out punctuation,\n",
    "    removes excess whitespace within a string & leading & trailing whitespace\"\"\"\n",
    "    new_list = []\n",
    "    table = string.maketrans(\"\",\"\")\n",
    "    for elem in data:\n",
    "        elem = \"\".join(i for i in elem if ord(i)<128)\n",
    "        elem = str(elem)        \n",
    "        elem = elem.lower()\n",
    "        # New addition to handle elipsis\n",
    "#         elem = re.sub('\\\\.+', ' ', elem)\n",
    "        elem = elem.translate(table, string.punctuation)\n",
    "        elem = re.sub(' +',' ', elem)\n",
    "        elem = elem.strip()\n",
    "        \n",
    "        new_list.append(elem)\n",
    "    return new_list\n",
    "\n",
    "#Clean entire data set at once\n",
    "data.escape = process_data(data.escape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull in GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the word list!\n",
      "Loaded the word vectors!\n"
     ]
    }
   ],
   "source": [
    "# Pull in word list & vectors\n",
    "wordsList = np.load('wordsList.npy')\n",
    "print('Loaded the word list!')\n",
    "wordsList = wordsList.tolist() #Originally loaded as numpy array\n",
    "wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "wordVectors = np.load('wordVectors.npy')\n",
    "print ('Loaded the word vectors!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load all of train and test data\n",
    "p = np.load('train_test.npz')\n",
    "train_pol_y = p['train_pol_y']\n",
    "test_pol_y = p['test_pol_y']\n",
    "train_pol_x = p['train_pol_x']\n",
    "test_pol_x = p['test_pol_x']\n",
    "train_emo = p['train_emo']\n",
    "test_emo = p['test_emo']\n",
    "train_emo_y = p['train_emo_y']\n",
    "tests_emo_y = p['tests_emo_y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get matrix ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Matrix ids for each tweet were built using GloVe word embeddings\n",
    "# Because construction of matrix ids is computationally expensive,\n",
    "# matrix ids were saved and will simply be reloaded\n",
    "d = np.load('ids.npz')\n",
    "train_ids = d['train_ids']\n",
    "test_ids = d['test_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16840, 31)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "# For Polarity Classifier\n",
    "def getTrainBatch(train_data, train_labels, train_ids):\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    # iterate through batch size\n",
    "    for i in range(batchSize):\n",
    "        num = randint(1, (len(train_data)-1))\n",
    "        if train_labels[num-1] == 1:\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "            \n",
    "        arr[i] = train_ids[num-1:num]\n",
    "        \n",
    "    return arr.astype(int), labels\n",
    "\n",
    "def getTestBatch(test_data, test_labels, test_ids):\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(1,(len(test_data)-1))\n",
    "        \n",
    "        if test_labels[num-1] == 1:\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "            \n",
    "        arr[i] = test_ids[num-1:num]\n",
    "        \n",
    "    return arr.astype(int), labels\n",
    "\n",
    "# For sub-emotion classifier\n",
    "# def getTrainBatch_subEmo(train_data, train_labels, train_ids, batchSize, maxSeqLength):\n",
    "#     labels = []\n",
    "#     arr = np.zeros([batchSize, maxSeqLength])\n",
    "#     # iterate through batch size\n",
    "#     for i in range(batchSize):\n",
    "#         num = randint(1, (len(train_data)-1))\n",
    "#         labels.append(train_labels[num-1])\n",
    "            \n",
    "#         arr[i] = train_ids[num-1:num]\n",
    "        \n",
    "#     return arr.astype(int), labels\n",
    "\n",
    "def getTrainBatch_subEmo(train_data, train_labels, train_ids, batchSize, maxSeqLength):\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    # iterate through batch size\n",
    "    for i in range(batchSize-10): #took out -5\n",
    "        num = randint(1, (len(train_data)-1))\n",
    "        labels.append(train_labels[num-1])\n",
    "            \n",
    "        arr[i] = train_ids[num-1:num]\n",
    "    \n",
    "    disgust = []\n",
    "    for m in range(len(train_labels)):\n",
    "        if train_labels[m][1] == 1:\n",
    "            disgust.append(m)\n",
    "    \n",
    "    for mel in range(5):\n",
    "        num = randint(1, (len(disgust)-1))\n",
    "        ind = disgust[num]\n",
    "        labels.append(train_labels[ind])\n",
    "        arr[batchSize-mel-1] = train_ids[ind]\n",
    "        \n",
    "    anger = []\n",
    "    for p in range(len(train_labels)):\n",
    "        if train_labels[p][0] == 1:\n",
    "            anger.append(p)\n",
    "    \n",
    "    for pri in range(5,10):\n",
    "        num = randint(1, (len(anger)-1))\n",
    "        ind = anger[num]\n",
    "        labels.append(train_labels[ind])\n",
    "        arr[batchSize-pri-1] = train_ids[ind]\n",
    "    \n",
    "    return arr.astype(int), labels\n",
    "\n",
    "\n",
    "def getTestBatch_subEmo(test_data, test_labels, test_ids, batchSize, maxSeqLength):\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(1,(len(test_data)-1))\n",
    "        labels.append(test_labels[num-1])\n",
    "            \n",
    "        arr[i] = test_ids[num-1:num]\n",
    "        \n",
    "    return arr.astype(int), labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub-emotion Classifier without polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Layer shape (75, 31, 50)\n"
     ]
    }
   ],
   "source": [
    "# Specify parameters\n",
    "\n",
    "maxSeqLength = max([len(elem.split()) for elem in data.ix[:, 4]]) #Maximum number of words in a tweet\n",
    "batchSize = 75\n",
    "hiddenStateSize = 1\n",
    "# lstmUnits = 2\n",
    "numClasses = 6\n",
    "numDimensions = 50\n",
    "keepProb = 0.5\n",
    "learningRate = 0.001\n",
    "\n",
    "iterations = 1500\n",
    "\n",
    "# Reset graph & create placeholders\n",
    "tf.reset_default_graph()\n",
    "labels = tf.placeholder(tf.int32, [batchSize, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])\n",
    "ns = tf.tile([maxSeqLength], [batchSize, ])\n",
    "\n",
    "# Lookup word vectors\n",
    "with tf.name_scope(\"Embedding_Layer\"):\n",
    "    data_vec = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "    data_vec = tf.nn.embedding_lookup(wordVectors,input_data)\n",
    "    \n",
    "    # I THINK WE HAVE TO EXTEND THE VECTOR RIGHT HERE\n",
    "\n",
    "\n",
    "# Construct RNN/LSTM cell and recurrent layer.\n",
    "with tf.name_scope(\"Cell_RNN_Layer\"):\n",
    "    lstmCell = tf.contrib.rnn.BasicLSTMCell(numDimensions, forget_bias=0.0)\n",
    "    lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, input_keep_prob=keepProb, output_keep_prob=keepProb)            \n",
    "    lstmCell = tf.contrib.rnn.MultiRNNCell([lstmCell] * hiddenStateSize)\n",
    "    value, _ = tf.nn.dynamic_rnn(lstmCell, data_vec, sequence_length=ns, dtype=tf.float32)\n",
    "\n",
    "    \n",
    "with tf.name_scope(\"Output_Layer\"):\n",
    "    weight = tf.Variable(tf.random_uniform([numDimensions, numClasses], -1.0, 1.0))\n",
    "    bias = tf.zeros(numClasses, tf.float32)\n",
    "    value = tf.transpose(value, [1, 0, 2])\n",
    "    last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "    multiplier = tf.matmul(last, weight)\n",
    "    prediction = tf.add(multiplier, bias)\n",
    "\n",
    "    print \"Embedding Layer shape\", data_vec.shape\n",
    "#     print \"Output of RNN shape\", value.shape\n",
    "#     print \"Weights shape\", weight.shape\n",
    "#     print \"Bias shape\", bias.shape\n",
    "#     print \"New shape for value\", value.shape\n",
    "#     print \"last shape\", last.shape\n",
    "#     print \"multiplier shape\", multiplier.shape\n",
    "#     print \"Output shape\", prediction.shape\n",
    "    \n",
    "with tf.name_scope(\"Prediction_Layer\"):\n",
    "    # Define correct predictions and accuracy\n",
    "    comparison = tf.argmax(prediction,1)\n",
    "    correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "\n",
    "    # Define loss & optimizer\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learningRate).minimize(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Next Batch of reviews\n",
    "    nextBatch, nextBatchLabels = getTrainBatch_subEmo(train_pol_x, train_emo_y, train_ids, batchSize, maxSeqLength);\n",
    "    sess.run(optimizer, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "   \n",
    "    # Write summary to Tensorboard\n",
    "    summary = sess.run(merged, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "    writer.add_summary(summary, i)\n",
    "\n",
    "#     # Save the network every 10,000 training iterations\n",
    "#     if (i % 10000 == 0 and i != 0):\n",
    "#         save_path = saver.save(sess, \"models/pretrained_lstm.ckpt\", global_step=i)\n",
    "#         print(\"saved to %s\" % save_path)\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy for this batch:', 54.666668176651001)\n",
      "('Accuracy for this batch:', 49.333333969116211)\n",
      "('Accuracy for this batch:', 38.666665554046631)\n",
      "('Accuracy for this batch:', 56.000000238418579)\n",
      "('Accuracy for this batch:', 45.333334803581238)\n",
      "('Accuracy for this batch:', 37.333333492279053)\n",
      "('Accuracy for this batch:', 40.000000596046448)\n",
      "('Accuracy for this batch:', 40.000000596046448)\n",
      "('Accuracy for this batch:', 47.999998927116394)\n",
      "('Accuracy for this batch:', 43.999999761581421)\n"
     ]
    }
   ],
   "source": [
    "iterations = 10\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = getTestBatch_subEmo(test_pol_x, tests_emo_y, test_ids, batchSize, maxSeqLength);\n",
    "    print(\"Accuracy for this batch:\", (sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Bias:', array([ 0.,  0.,  0.,  0.,  0.,  0.], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "print(\"Bias:\", (sess.run(bias, {input_data: nextBatch, labels: nextBatchLabels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Compare:', array([3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3]))\n"
     ]
    }
   ],
   "source": [
    "# Show index of predicted class\n",
    "print(\"Compare:\", (sess.run(comparison, {input_data: nextBatch, labels: nextBatchLabels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Preds:', array([[ -1.70764267e-01,  -1.18983972e+00,  -5.41362107e-01,\n",
      "          6.36327565e-01,   3.38299751e-01,   1.79633245e-01],\n",
      "       [ -1.19698989e+00,  -2.05328321e+00,  -2.01932818e-01,\n",
      "          1.36098087e+00,   6.58866107e-01,   6.26864076e-01],\n",
      "       [ -1.37206626e+00,  -2.97011209e+00,  -1.14057708e+00,\n",
      "          2.07653046e+00,   1.22284031e+00,   8.62038255e-01],\n",
      "       [ -4.96542275e-01,  -8.49670291e-01,   1.07648514e-01,\n",
      "          7.06201136e-01,   2.07316279e-01,   2.47223362e-01],\n",
      "       [ -1.56369805e+00,  -2.18205690e+00,  -5.20198703e-01,\n",
      "          1.83059955e+00,   4.45347369e-01,   7.49708951e-01],\n",
      "       [ -4.74655926e-01,  -1.14407694e+00,  -3.83297876e-02,\n",
      "          7.92131722e-01,   3.42834473e-01,   3.81663144e-01],\n",
      "       [ -6.71388924e-01,  -1.65920734e+00,  -7.78439105e-01,\n",
      "          1.56935143e+00,   8.43184590e-01,   9.76065993e-01],\n",
      "       [ -9.34223294e-01,  -1.18638587e+00,  -4.24825251e-01,\n",
      "          1.16256618e+00,   1.75717831e-01,   2.98819005e-01],\n",
      "       [ -3.25309008e-01,  -3.43383104e-01,   1.36539340e-01,\n",
      "          2.59446889e-01,   2.62829438e-02,   5.54630421e-02],\n",
      "       [ -1.00302565e+00,  -2.75982809e+00,  -3.66267264e-01,\n",
      "          1.98584998e+00,   1.92215875e-01,   9.33317304e-01],\n",
      "       [ -8.27580929e-01,  -8.44062090e-01,  -4.60650921e-01,\n",
      "          1.04945946e+00,   1.96608022e-01,   1.27833962e-01],\n",
      "       [ -1.56971300e+00,  -2.05792427e+00,  -8.38024855e-01,\n",
      "          1.46582198e+00,   1.09834182e+00,   5.80936790e-01],\n",
      "       [ -1.57547116e-01,  -1.76386699e-01,   2.90120006e-01,\n",
      "          6.87774122e-02,   1.05716430e-01,  -1.25956059e-01],\n",
      "       [ -1.11339128e+00,  -1.48619735e+00,  -4.48572040e-01,\n",
      "          1.10155249e+00,   3.76803577e-01,   1.61569729e-01],\n",
      "       [ -1.06056750e+00,  -1.72013736e+00,  -2.91008055e-01,\n",
      "          1.85729325e+00,   2.14286461e-01,   3.15440834e-01],\n",
      "       [ -2.63751239e-01,  -3.29569578e-01,   2.49633253e-01,\n",
      "          1.54573768e-01,   5.46791479e-02,   4.27660160e-03],\n",
      "       [ -5.46592593e-01,  -6.77981675e-01,   9.44648013e-02,\n",
      "          4.35825080e-01,   9.90180969e-02,  -1.28561378e-01],\n",
      "       [ -5.93720853e-01,  -9.45362329e-01,   1.61826462e-01,\n",
      "          6.42394006e-01,   2.09490359e-01,   2.08240896e-01],\n",
      "       [ -9.67370570e-01,  -1.63804722e+00,  -3.67701948e-01,\n",
      "          1.17831135e+00,   1.05361521e-01,   6.39029503e-01],\n",
      "       [ -1.37315607e+00,  -2.53479171e+00,  -6.15997277e-02,\n",
      "          2.76456499e+00,   6.14803493e-01,   1.27962661e+00],\n",
      "       [ -1.89132482e-01,  -1.62040368e-01,   2.44463652e-01,\n",
      "          7.24049732e-02,  -2.49905325e-02,  -1.62590027e-01],\n",
      "       [ -8.71175647e-01,  -1.07875431e+00,  -2.95331538e-01,\n",
      "          6.02060556e-01,   3.94683897e-01,  -1.04154795e-02],\n",
      "       [ -1.28259504e+00,  -1.85532689e+00,  -5.83024085e-01,\n",
      "          1.82285678e+00,   6.76888693e-03,   3.81417900e-01],\n",
      "       [ -5.70033729e-01,  -9.49328423e-01,  -1.78272754e-01,\n",
      "          3.70725870e-01,   2.31986612e-01,   1.52686760e-01],\n",
      "       [ -4.46536005e-01,  -6.08572662e-01,  -1.00045525e-01,\n",
      "          2.94848621e-01,   7.79292881e-02,   1.41799375e-01],\n",
      "       [ -1.11933422e+00,  -1.45720911e+00,   1.51976645e-01,\n",
      "          1.30122662e+00,   3.92532319e-01,   7.99140036e-01],\n",
      "       [ -3.56918156e-01,  -5.41878998e-01,  -1.71345264e-01,\n",
      "          4.25303638e-01,   6.68539107e-02,   1.71057940e-01],\n",
      "       [ -3.04720759e-01,  -4.98475939e-01,   1.22661471e-01,\n",
      "          3.15353215e-01,   1.96857005e-01,  -1.84553694e-02],\n",
      "       [ -8.06526601e-01,  -1.71187592e+00,   1.87136978e-02,\n",
      "          7.62700260e-01,   3.71858686e-01,   2.35362887e-01],\n",
      "       [ -7.59310484e-01,  -1.70617890e+00,   5.08259088e-02,\n",
      "          1.43390656e+00,   4.14030313e-01,   6.65072620e-01],\n",
      "       [ -1.70272851e+00,  -3.82253385e+00,  -1.10921574e+00,\n",
      "          2.48471880e+00,   7.30646014e-01,   8.49982798e-01],\n",
      "       [ -2.92591631e-01,  -2.87758082e-01,   1.15544945e-02,\n",
      "          2.27144867e-01,   1.42019540e-01,   5.82979713e-03],\n",
      "       [ -6.40675247e-01,  -1.18925822e+00,  -9.83484462e-03,\n",
      "          1.06849575e+00,   2.25706339e-01,   6.57466173e-01],\n",
      "       [ -3.24719310e-01,  -4.64520216e-01,  -8.40920024e-03,\n",
      "          3.28265369e-01,   1.23977520e-01,  -9.76078212e-04],\n",
      "       [ -5.50058484e-01,  -6.82020664e-01,  -1.51069909e-01,\n",
      "          4.00899410e-01,   1.96060628e-01,   6.70832992e-02],\n",
      "       [ -7.73166656e-01,  -2.14611721e+00,  -4.96523499e-01,\n",
      "          2.03172064e+00,   1.11273968e+00,   6.07274771e-01],\n",
      "       [ -5.72834015e-01,  -5.87713242e-01,  -4.34997082e-01,\n",
      "          7.14839101e-01,   1.74016505e-01,   1.96864903e-01],\n",
      "       [ -1.93240678e+00,  -2.47082686e+00,  -1.21847284e+00,\n",
      "          1.14379036e+00,   1.32894397e-01,   4.40904766e-01],\n",
      "       [ -3.52858037e-01,  -1.36837196e+00,  -4.46545988e-01,\n",
      "          1.36402512e+00,   5.62998414e-01,   3.39123577e-01],\n",
      "       [ -1.90412009e+00,  -3.12960815e+00,  -1.48524714e+00,\n",
      "          2.85108519e+00,   6.65380657e-01,   9.01800334e-01],\n",
      "       [ -1.18548667e+00,  -4.04352283e+00,  -1.10618281e+00,\n",
      "          2.74870157e+00,   7.75013149e-01,   1.76462674e+00],\n",
      "       [ -3.99978966e-01,  -1.37556040e+00,  -2.63437152e-01,\n",
      "          8.53171229e-01,   1.19010054e-01,   1.54416934e-01],\n",
      "       [ -8.22391570e-01,  -1.48021197e+00,   5.21895103e-02,\n",
      "          1.19677925e+00,  -1.01309232e-02,   6.49726927e-01],\n",
      "       [ -6.09087765e-01,  -7.98921406e-01,   2.04753637e-01,\n",
      "          8.98832858e-01,   3.80960822e-01,   2.77758956e-01],\n",
      "       [ -5.58798194e-01,  -7.13655174e-01,  -1.70106426e-01,\n",
      "          6.03441179e-01,   1.86750591e-01,   6.59920350e-02],\n",
      "       [ -1.12576032e+00,  -2.40236616e+00,  -8.98386180e-01,\n",
      "          1.74980223e+00,   7.40805507e-01,   7.24535286e-01],\n",
      "       [  1.39359497e-02,   1.98825136e-01,   4.29190755e-01,\n",
      "         -4.93926615e-01,  -3.54906768e-01,  -1.84457600e-01],\n",
      "       [ -6.05879873e-02,   5.09942435e-02,   1.29495993e-01,\n",
      "         -1.61813572e-01,  -2.21553609e-01,  -5.69246151e-02],\n",
      "       [ -1.67958927e+00,  -3.13854074e+00,  -8.59297752e-01,\n",
      "          2.52405334e+00,   1.50371623e+00,   1.21682048e+00],\n",
      "       [ -3.60265613e-01,  -6.80601060e-01,   1.00992121e-01,\n",
      "          3.74916852e-01,   6.92617744e-02,  -6.91561699e-02],\n",
      "       [ -3.55088651e-01,  -4.31960136e-01,   1.60391331e-02,\n",
      "          3.42218310e-01,   1.77794546e-01,   1.58063531e-01],\n",
      "       [ -2.33208227e+00,  -3.23459077e+00,  -4.04958762e-02,\n",
      "          1.86679387e+00,   2.47689545e-01,   1.64540112e+00],\n",
      "       [ -1.12433743e+00,  -2.26691818e+00,  -3.51526976e-01,\n",
      "          1.63580561e+00,   1.02622539e-01,   6.09419107e-01],\n",
      "       [ -9.27446544e-01,  -2.10442352e+00,  -3.82840097e-01,\n",
      "          1.83540344e+00,   4.78572249e-01,   6.74057245e-01],\n",
      "       [ -3.16683590e-01,  -5.54788351e-01,  -6.51835352e-02,\n",
      "          3.59358132e-01,   2.39561394e-01,   1.56514436e-01],\n",
      "       [ -7.78688371e-01,  -1.42952991e+00,   1.34841293e-01,\n",
      "          9.06917095e-01,   1.12071931e-01,   3.45804036e-01],\n",
      "       [ -5.81654072e-01,  -1.39833236e+00,  -6.46981537e-01,\n",
      "          9.80237901e-01,   6.73370719e-01,   5.78242421e-01],\n",
      "       [ -2.92064518e-01,  -5.08345902e-01,   7.89032206e-02,\n",
      "          2.42876589e-01,   1.03146322e-02,   4.25573140e-02],\n",
      "       [ -9.94099081e-01,  -2.87407255e+00,  -1.44328225e+00,\n",
      "          2.44776368e+00,   1.26321173e+00,   3.93327773e-01],\n",
      "       [ -2.31997654e-01,  -1.83487579e-01,   3.02265167e-01,\n",
      "          1.22112557e-02,  -9.26016048e-02,  -2.39514112e-02],\n",
      "       [ -5.23647845e-01,  -7.36150682e-01,  -1.29476096e-03,\n",
      "          4.63968605e-01,   4.89933789e-03,   1.39463052e-01],\n",
      "       [ -9.29791689e-01,  -1.77445543e+00,  -2.22933665e-01,\n",
      "          1.10560536e+00,   5.12261763e-02,   2.19037071e-01],\n",
      "       [ -1.02567029e+00,  -1.71572959e+00,  -7.86959052e-01,\n",
      "          1.03093624e+00,   1.07945740e+00,   2.80247211e-01],\n",
      "       [ -3.35270077e-01,  -9.03541625e-01,   7.53370896e-02,\n",
      "          7.60030150e-01,   2.00194255e-01,   1.56767458e-01],\n",
      "       [ -1.30511415e+00,  -2.92817354e+00,   3.48239064e-01,\n",
      "          2.92753267e+00,   5.45940340e-01,   1.47833633e+00],\n",
      "       [ -1.56212378e+00,  -2.93117619e+00,  -1.18883228e+00,\n",
      "          1.50580859e+00,  -3.46374512e-01,   4.86944526e-01],\n",
      "       [ -6.16717219e-01,  -2.87530494e+00,  -4.57378834e-01,\n",
      "          1.63156927e+00,   5.72723567e-01,   8.21580589e-01],\n",
      "       [ -6.70085013e-01,  -1.11917436e+00,  -4.23341423e-01,\n",
      "          5.54088652e-01,   4.34490681e-01,   4.09076750e-01],\n",
      "       [  2.35389844e-01,   1.94741860e-01,   4.35273916e-01,\n",
      "         -2.61576325e-01,   4.99727502e-02,  -3.63742501e-01],\n",
      "       [ -5.96381068e-01,  -1.29803872e+00,  -3.60802770e-01,\n",
      "          7.40934193e-01,   3.13325644e-01,   1.89109176e-01],\n",
      "       [ -3.41934949e-01,  -5.86118698e-01,   5.62736057e-02,\n",
      "          2.86699384e-01,   7.29351342e-02,   5.16068116e-02],\n",
      "       [ -6.94184124e-01,  -1.17568326e+00,   2.80405343e-01,\n",
      "          9.74441648e-01,   1.89091370e-01,   4.29495960e-01],\n",
      "       [  2.08145976e-01,  -2.90471464e-02,   6.01073146e-01,\n",
      "         -6.59629881e-01,  -4.52607334e-01,  -3.78406167e-01],\n",
      "       [ -8.27379942e-01,  -1.29239690e+00,  -4.41496104e-01,\n",
      "          1.59825861e+00,   3.67606938e-01,   3.09059411e-01],\n",
      "       [ -2.11934662e+00,  -3.48566008e+00,  -4.94924784e-01,\n",
      "          2.03669357e+00,   7.80403316e-02,   4.60750431e-01]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "# Show actual predicted values\n",
    "print(\"Preds:\", (sess.run(prediction, {input_data: nextBatch, labels: nextBatchLabels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tests_emo_y[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
